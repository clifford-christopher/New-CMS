# Story 4.3: News Generation per Prompt Type with Adaptive Routing

## Status

Draft

## Version

2.0

## Story

**As a** content manager,
**I want** to generate news for each checked prompt type using adaptive routing,
**so that** I can test different prompts while maintaining backward compatibility with legacy generation.

## Acceptance Criteria

1. "Generate News" button triggers backend endpoint `POST /api/news/generate` with trigger_name, stockid, prompt_type
2. Backend checks `trigger_prompts.isActive` flag for adaptive routing (Story 4.1)
3. If isActive=false: Use legacy 3-prompt method (existing `generate_news_og.py`)
4. If isActive=true: Fetch merged data (OLD/NEW/OLD_NEW from Story 2.3), generate with single HTML prompt
5. Frontend loops through checked prompt types (üí∞ Paid, üÜì Unpaid, üï∑Ô∏è Crawler) - one API call per type
6. Status indicators grouped by prompt type (not by model for MVP): Pending, Generating, Complete, Failed
7. Progress display: "Generating 2 of 3 prompt types complete"
8. Generation completes within NFR3: 30 seconds timeout per generation (configurable)
9. Failed generations display error message without blocking successful ones (which prompt type failed)
10. Results stored in `generation_history` collection with trigger_name, stockid, prompt_type, data_mode, method (new/legacy)
11. Results grouped by prompt type in UI: [üí∞ Paid] [üÜì Unpaid] [üï∑Ô∏è Crawler] sections with colored headers
12. Metadata displayed per generation: Tokens | Time | Cost | Method (New/Legacy)
13. Meets FR19, FR20: generation per prompt type with status indicators
14. Session history allows navigating back to previous results with prompt type context

## Tasks / Subtasks

- [ ] Task 1: Create GenerationService for orchestrating parallel LLM calls (AC: 2, 3, 8, 9)
  - [ ] Create backend/app/services/generation_service.py
  - [ ] Implement async generate_parallel method using asyncio.gather
  - [ ] Handle (models √ó checked types) calculation
  - [ ] Implement timeout per model (30 seconds default)
  - [ ] Error isolation: one failure doesn't block others
  - [ ] Return results grouped by prompt type ‚Üí model
- [ ] Task 2: Create backend generation endpoint with SSE support (AC: 1, 5, 6)
  - [ ] Create POST /api/news/generate endpoint
  - [ ] Accept request: trigger_name, stockid, data_mode, sections, section_order, checked prompt types
  - [ ] Implement Server-Sent Events (SSE) for real-time status updates
  - [ ] Stream status updates: Pending ‚Üí Generating ‚Üí Complete/Failed
  - [ ] Include prompt type and model in each update
  - [ ] Final response includes all results grouped by type
- [ ] Task 3: Store generation results in MongoDB (AC: 10, 11)
  - [ ] Create generation_history collection schema
  - [ ] Store generation metadata: trigger_id, stock_id, timestamp, models, prompt_types
  - [ ] Store prompts per type: {paid: "...", unpaid: "..."}
  - [ ] Store results per type per model: {paid: {gpt-4: {...}, claude: {...}}, unpaid: {...}}
  - [ ] Implement session history retrieval endpoint
- [ ] Task 4: Create GenerationContext for frontend state management (AC: 4, 7)
  - [ ] Create frontend/src/contexts/GenerationContext.tsx
  - [ ] State for generation status per type per model
  - [ ] State for results grouped by type ‚Üí model
  - [ ] State for progress tracking (X of Y complete)
  - [ ] Methods: startGeneration, updateStatus, cancelGeneration
- [ ] Task 5: Create GenerationPanel component with status indicators (AC: 4, 7, 9)
  - [ ] Create frontend/src/components/config/GenerationPanel.tsx
  - [ ] "Generate News" button (enabled when models selected and prompts exist)
  - [ ] Real-time status display grouped by prompt type
  - [ ] Status badges per model: Pending (gray), Generating (blue), Complete (green), Failed (red)
  - [ ] Progress bar with count: "Generating 4 of 6 complete"
  - [ ] Error messages displayed inline per failed generation
- [ ] Task 6: Implement SSE client or polling mechanism (AC: 5)
  - [ ] Create frontend/src/lib/sse-client.ts for SSE handling
  - [ ] EventSource API for SSE connection
  - [ ] Fallback to polling if SSE not supported
  - [ ] Handle connection errors and reconnection
  - [ ] Update GenerationContext with status updates
- [ ] Task 7: Write unit and integration tests
  - [ ] Test GenerationService parallel execution
  - [ ] Test timeout handling per model
  - [ ] Test error isolation (one failure doesn't block others)
  - [ ] Test SSE endpoint
  - [ ] Test GenerationContext state updates
  - [ ] Test GenerationPanel UI rendering

## Dev Notes

### Prerequisites from Previous Stories

[Source: Story 4.1 - Completion, Story 4.2 - Completion]

Before starting this story, ensure the following stories are complete:

**Epic 2 Dependencies (v2.0)**:
- Story 2.3 v2.0: Merged data structure with OLD/NEW/OLD_NEW modes
- Story 2.4 v2.0: Data schema and transformation pipeline
- Story 2.5 v2.0: DataContext provides structured data for prompt substitution

**Epic 3 Dependencies (v2.0)**:
- Story 3.2 v2.0: PromptContext provides prompts for all checked types (paid/unpaid/crawler)
- checkedTypes Set<string> contains active prompt types

**Epic 4 Dependencies (v2.0)**:
- Story 4.1 v2.0: LLM provider abstraction layer with OpenAI, Anthropic, Google providers
- Story 4.1 v2.0: Provider registry for dynamic model lookup
- Story 4.1 v2.0: Cost calculation per provider
- ModelContext with selected models, temperature, max_tokens (Story 4.2)

### Project Structure & File Locations

[Source: architecture.md - Source Tree and Module Organization]

**Backend files to create** in `backend/app/`:
```
backend/app/
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îî‚îÄ‚îÄ generation.py                   # LLM generation endpoints (NEW)
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ generation_service.py           # Parallel generation orchestration (NEW)
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ generation.py                   # Generation request/response models (NEW)
```

**Frontend files to create** in `frontend/src/`:
```
frontend/src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ GenerationPanel.tsx         # Generation UI with status (NEW)
‚îÇ       ‚îú‚îÄ‚îÄ GenerationStatus.tsx        # Status indicator component (NEW)
‚îÇ       ‚îî‚îÄ‚îÄ ProgressTracker.tsx         # Progress bar component (NEW)
‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îî‚îÄ‚îÄ GenerationContext.tsx           # Generation state management (NEW)
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îî‚îÄ‚îÄ sse-client.ts                   # SSE client implementation (NEW)
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ generation.ts                   # TypeScript types for generation (NEW)
```

### Technology Stack Details

[Source: architecture.md - High Level Architecture - Planned Tech Stack]

**Backend Stack for Parallel Generation**:
- **Python asyncio**: Async/await for parallel LLM API calls
- **asyncio.gather**: Execute multiple API calls concurrently
- **FastAPI SSE**: Server-Sent Events for real-time updates
- **Motor**: Async MongoDB driver for storing generation history
- **Provider Registry**: From Story 4.1 for dynamic model lookup

**Frontend Stack**:
- **EventSource API**: Browser native SSE client
- **React Context API**: GenerationContext for state management
- **React-Bootstrap**: Progress bars, badges, alerts
- **TypeScript**: Type safety for generation operations

**Install Dependencies**:
```bash
# Backend
pip install asyncio  # Built-in with Python 3.11+

# Frontend (already included in Story 1.3)
npm install  # react-bootstrap already available
```

### Backend Generation Service Implementation

[Source: architecture.md - Epic 4 - Multi-Model Generation & Testing]

**File: backend/app/services/generation_service.py** (NEW)

Complete parallel generation orchestration:

```python
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
from ..llm_providers.registry import ProviderRegistry
from ..config import LLM_API_KEYS

logger = logging.getLogger(__name__)

class GenerationService:
    """
    Service for orchestrating parallel LLM generation across multiple models and prompt types
    """

    def __init__(self, timeout: float = 30.0):
        """
        Initialize generation service

        Args:
            timeout: Timeout per model generation in seconds (default: 30)
        """
        self.timeout = timeout
        self.provider_registry = ProviderRegistry

    async def generate_parallel(
        self,
        trigger_id: str,
        stock_id: str,
        selected_models: List[str],
        prompt_types: List[str],
        prompts: Dict[str, str],
        structured_data: Dict[str, Any],
        model_settings: Dict[str, Any],
        status_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Generate news in parallel for all (model √ó prompt type) combinations

        Args:
            trigger_id: Trigger identifier
            stock_id: Stock identifier for context
            selected_models: List of model IDs to use
            prompt_types: List of checked prompt types (e.g., ["paid", "unpaid"])
            prompts: Dict mapping prompt type to prompt text
            structured_data: Structured data for prompt substitution
            model_settings: Settings (temperature, max_tokens)
            status_callback: Optional callback for status updates

        Returns:
            Dict containing:
                - generation_id: Unique generation ID
                - results: Dict[prompt_type][model_id] -> generation result
                - metadata: Generation metadata (timestamp, counts, errors)
        """
        generation_id = f"{trigger_id}_{stock_id}_{int(datetime.utcnow().timestamp())}"

        logger.info(
            f"Starting parallel generation: {len(selected_models)} models √ó "
            f"{len(prompt_types)} types = {len(selected_models) * len(prompt_types)} total"
        )

        # Calculate total generations
        total_generations = len(selected_models) * len(prompt_types)
        completed_generations = 0
        failed_generations = 0

        # Results structure: {prompt_type: {model_id: result}}
        results: Dict[str, Dict[str, Any]] = {
            ptype: {} for ptype in prompt_types
        }

        # Create generation tasks for all combinations
        tasks = []
        task_metadata = []  # Track which task corresponds to which (type, model)

        for prompt_type in prompt_types:
            prompt = prompts.get(prompt_type)
            if not prompt:
                logger.warning(f"No prompt found for type: {prompt_type}")
                continue

            for model_id in selected_models:
                # Create task for this (type, model) combination
                task = self._generate_single(
                    model_id=model_id,
                    prompt=prompt,
                    model_settings=model_settings,
                    prompt_type=prompt_type,
                    status_callback=status_callback
                )
                tasks.append(task)
                task_metadata.append({
                    "prompt_type": prompt_type,
                    "model_id": model_id
                })

        # Execute all tasks in parallel using asyncio.gather
        # return_exceptions=True ensures one failure doesn't stop others
        task_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        for idx, result in enumerate(task_results):
            meta = task_metadata[idx]
            prompt_type = meta["prompt_type"]
            model_id = meta["model_id"]

            if isinstance(result, Exception):
                # Generation failed
                logger.error(
                    f"Generation failed for {prompt_type}/{model_id}: {result}"
                )
                results[prompt_type][model_id] = {
                    "status": "failed",
                    "error": str(result),
                    "generated_text": None,
                    "token_count": 0,
                    "cost": 0.0,
                    "latency": 0.0
                }
                failed_generations += 1

                # Send status update
                if status_callback:
                    await status_callback({
                        "prompt_type": prompt_type,
                        "model_id": model_id,
                        "status": "failed",
                        "error": str(result)
                    })
            else:
                # Generation succeeded
                results[prompt_type][model_id] = {
                    "status": "complete",
                    "error": None,
                    **result  # Include all provider response fields
                }
                completed_generations += 1

                # Send status update
                if status_callback:
                    await status_callback({
                        "prompt_type": prompt_type,
                        "model_id": model_id,
                        "status": "complete",
                        "result": result
                    })

        logger.info(
            f"Parallel generation complete: {completed_generations} succeeded, "
            f"{failed_generations} failed out of {total_generations} total"
        )

        return {
            "generation_id": generation_id,
            "results": results,
            "metadata": {
                "trigger_id": trigger_id,
                "stock_id": stock_id,
                "timestamp": datetime.utcnow().isoformat(),
                "total_generations": total_generations,
                "completed": completed_generations,
                "failed": failed_generations,
                "prompt_types": prompt_types,
                "selected_models": selected_models
            }
        }

    async def _generate_single(
        self,
        model_id: str,
        prompt: str,
        model_settings: Dict[str, Any],
        prompt_type: str,
        status_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        Generate text for a single (model, prompt) combination with timeout

        Args:
            model_id: Model identifier
            prompt: Prompt text
            model_settings: Temperature, max_tokens
            prompt_type: Type of prompt (for logging)
            status_callback: Optional callback for status updates

        Returns:
            Generation result from provider
        """
        # Send "generating" status
        if status_callback:
            await status_callback({
                "prompt_type": prompt_type,
                "model_id": model_id,
                "status": "generating"
            })

        # Get API key for provider
        provider_name = self._get_provider_name(model_id)
        api_key = LLM_API_KEYS.get(provider_name.lower())

        if not api_key:
            raise ValueError(f"No API key found for provider: {provider_name}")

        # Get provider instance
        provider = self.provider_registry.get_provider(
            model_id=model_id,
            api_key=api_key,
            config={
                "timeout": self.timeout,
                **model_settings
            }
        )

        if not provider:
            raise ValueError(f"Provider not found for model: {model_id}")

        # Get full model name for API
        model_name = self.provider_registry.get_model_name(model_id)

        # Execute generation with timeout
        try:
            result = await asyncio.wait_for(
                provider.generate(
                    prompt=prompt,
                    model=model_name,
                    **model_settings
                ),
                timeout=self.timeout
            )
            return result
        except asyncio.TimeoutError:
            raise TimeoutError(
                f"Generation timeout after {self.timeout}s for model {model_id}"
            )

    def _get_provider_name(self, model_id: str) -> str:
        """Get provider name from model ID"""
        if model_id.startswith("gpt"):
            return "OpenAI"
        elif model_id.startswith("claude"):
            return "Anthropic"
        elif model_id.startswith("gemini"):
            return "Google"
        else:
            raise ValueError(f"Unknown provider for model: {model_id}")
```

### Backend API Endpoint with SSE

[Source: architecture.md - Data Models and APIs - Epic 4 APIs]

**File: backend/app/routers/generation.py** (NEW)

Generation endpoint with Server-Sent Events:

```python
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from typing import List, Dict, Any
from pydantic import BaseModel
from ..database import get_database
from ..services.generation_service import GenerationService
import asyncio
import json
import logging

router = APIRouter(prefix="/api/news", tags=["generation"])
logger = logging.getLogger(__name__)

class GenerationRequest(BaseModel):
    trigger_name: str
    stockid: str
    data_mode: str
    sections: List[str]
    section_order: List[str]
    prompt_types: List[str]
    selected_models: List[str]
    prompts: Dict[str, str]
    structured_data: Dict[str, Any]
    model_settings: Dict[str, Any]

@router.post("/generate")
async def generate_news(
    request: GenerationRequest,
    db = Depends(get_database)
):
    """
    Generate news using selected models for checked prompt types
    Supports Server-Sent Events (SSE) for real-time status updates

    Request body:
    {
        "trigger_name": "earnings_report",
        "stockid": "AAPL",
        "data_mode": "OLD_NEW",
        "sections": ["headline", "body", "analysis"],
        "section_order": ["headline", "body", "analysis"],
        "prompt_types": ["paid", "unpaid"],
        "selected_models": ["gpt-4", "claude-3-sonnet"],
        "prompts": {
            "paid": "Generate paid subscriber news...",
            "unpaid": "Generate free tier news..."
        },
        "structured_data": {...},
        "model_settings": {
            "temperature": 0.7,
            "max_tokens": 500
        }
    }

    Response (SSE stream):
    event: status
    data: {"prompt_type": "paid", "model_id": "gpt-4", "status": "generating"}

    event: status
    data: {"prompt_type": "paid", "model_id": "gpt-4", "status": "complete", "result": {...}}

    event: complete
    data: {"generation_id": "...", "results": {...}, "metadata": {...}}
    """

    async def event_generator():
        """Generate SSE events for status updates"""
        status_queue = asyncio.Queue()

        async def status_callback(status_update: Dict):
            """Callback to queue status updates"""
            await status_queue.put(status_update)

        # Start generation in background
        service = GenerationService(timeout=30.0)
        generation_task = asyncio.create_task(
            service.generate_parallel(
                trigger_name=request.trigger_name,
                stockid=request.stockid,
                data_mode=request.data_mode,
                sections=request.sections,
                section_order=request.section_order,
                selected_models=request.selected_models,
                prompt_types=request.prompt_types,
                prompts=request.prompts,
                structured_data=request.structured_data,
                model_settings=request.model_settings,
                status_callback=status_callback
            )
        )

        # Stream status updates while generation is running
        try:
            while not generation_task.done():
                try:
                    # Wait for status update with timeout
                    status_update = await asyncio.wait_for(
                        status_queue.get(),
                        timeout=0.5
                    )

                    # Send SSE event
                    event_data = json.dumps(status_update)
                    yield f"event: status\ndata: {event_data}\n\n"

                except asyncio.TimeoutError:
                    # No status update, continue waiting
                    continue

            # Generation complete, get result
            result = await generation_task

            # Store in generation_history
            await db.generation_history.insert_one({
                "generation_id": result["generation_id"],
                "trigger_name": request.trigger_name,
                "stockid": request.stockid,
                "data_mode": request.data_mode,
                "sections": request.sections,
                "section_order": request.section_order,
                "timestamp": result["metadata"]["timestamp"],
                "prompt_types": request.prompt_types,
                "prompts": request.prompts,
                "selected_models": request.selected_models,
                "model_settings": request.model_settings,
                "results": result["results"],
                "metadata": result["metadata"]
            })

            # Send final result
            event_data = json.dumps(result)
            yield f"event: complete\ndata: {event_data}\n\n"

        except Exception as e:
            logger.error(f"Generation error: {e}")
            error_data = json.dumps({"error": str(e)})
            yield f"event: error\ndata: {error_data}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )

@router.get("/generation-history")
async def get_generation_history(
    trigger_name: str,
    limit: int = 10,
    db = Depends(get_database)
):
    """
    Get generation history for a trigger
    Returns last N generations with session context
    """
    history = await db.generation_history.find(
        {"trigger_name": trigger_name}
    ).sort("timestamp", -1).limit(limit).to_list(length=limit)

    return {
        "trigger_name": trigger_name,
        "history": history
    }
```

### Frontend Generation Context

[Source: architecture.md - Source Tree and Module Organization - React Context API]

**File: frontend/src/contexts/GenerationContext.tsx** (NEW)

Complete generation state management:

```typescript
'use client';

import React, { createContext, useContext, useState, ReactNode, useCallback } from 'react';

type GenerationStatus = 'pending' | 'generating' | 'complete' | 'failed';

interface GenerationResult {
  status: GenerationStatus;
  error?: string;
  generated_text?: string;
  token_count?: number;
  cost?: number;
  latency?: number;
  timestamp?: string;
}

interface GenerationState {
  // Status per type per model: {paid: {gpt-4: 'generating', claude: 'pending'}}
  statuses: Record<string, Record<string, GenerationStatus>>;
  // Results per type per model
  results: Record<string, Record<string, GenerationResult>>;
  // Progress tracking
  totalGenerations: number;
  completedGenerations: number;
  failedGenerations: number;
  isGenerating: boolean;
  generationId: string | null;
}

interface GenerationContextType extends GenerationState {
  startGeneration: (
    triggerId: string,
    stockId: string,
    selectedModels: string[],
    promptTypes: string[],
    prompts: Record<string, string>,
    structuredData: any,
    modelSettings: any
  ) => Promise<void>;
  updateStatus: (promptType: string, modelId: string, status: GenerationStatus) => void;
  updateResult: (promptType: string, modelId: string, result: GenerationResult) => void;
  cancelGeneration: () => void;
  clearResults: () => void;
}

const GenerationContext = createContext<GenerationContextType | undefined>(undefined);

export function GenerationProvider({ children }: { children: ReactNode }) {
  const [state, setState] = useState<GenerationState>({
    statuses: {},
    results: {},
    totalGenerations: 0,
    completedGenerations: 0,
    failedGenerations: 0,
    isGenerating: false,
    generationId: null
  });

  const [eventSource, setEventSource] = useState<EventSource | null>(null);

  const startGeneration = useCallback(
    async (
      triggerId: string,
      stockId: string,
      selectedModels: string[],
      promptTypes: string[],
      prompts: Record<string, string>,
      structuredData: any,
      modelSettings: any
    ) => {
      // Initialize statuses
      const initialStatuses: Record<string, Record<string, GenerationStatus>> = {};
      const initialResults: Record<string, Record<string, GenerationResult>> = {};

      promptTypes.forEach(type => {
        initialStatuses[type] = {};
        initialResults[type] = {};
        selectedModels.forEach(model => {
          initialStatuses[type][model] = 'pending';
          initialResults[type][model] = { status: 'pending' };
        });
      });

      const totalGenerations = selectedModels.length * promptTypes.length;

      setState({
        statuses: initialStatuses,
        results: initialResults,
        totalGenerations,
        completedGenerations: 0,
        failedGenerations: 0,
        isGenerating: true,
        generationId: null
      });

      // Close existing EventSource if any
      if (eventSource) {
        eventSource.close();
      }

      // Create EventSource for SSE
      const url = `/api/news/generate`;
      const es = new EventSource(url);

      // Send POST request with data
      fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          trigger_name: triggerId,
          stockid: stockId,
          data_mode: 'OLD_NEW',  // From DataContext
          sections: [],  // From configuration
          section_order: [],  // From configuration
          selected_models: selectedModels,
          prompt_types: promptTypes,
          prompts,
          structured_data: structuredData,
          model_settings: modelSettings
        })
      });

      // Handle status events
      es.addEventListener('status', (event) => {
        const update = JSON.parse(event.data);
        const { prompt_type, model_id, status, result, error } = update;

        setState(prev => ({
          ...prev,
          statuses: {
            ...prev.statuses,
            [prompt_type]: {
              ...prev.statuses[prompt_type],
              [model_id]: status
            }
          },
          results: {
            ...prev.results,
            [prompt_type]: {
              ...prev.results[prompt_type],
              [model_id]: result
                ? { status, ...result }
                : { status, error }
            }
          },
          completedGenerations: status === 'complete'
            ? prev.completedGenerations + 1
            : prev.completedGenerations,
          failedGenerations: status === 'failed'
            ? prev.failedGenerations + 1
            : prev.failedGenerations
        }));
      });

      // Handle completion event
      es.addEventListener('complete', (event) => {
        const finalResult = JSON.parse(event.data);

        setState(prev => ({
          ...prev,
          isGenerating: false,
          generationId: finalResult.generation_id,
          results: finalResult.results,
          completedGenerations: finalResult.metadata.completed,
          failedGenerations: finalResult.metadata.failed
        }));

        es.close();
        setEventSource(null);
      });

      // Handle errors
      es.addEventListener('error', (event) => {
        console.error('SSE error:', event);
        setState(prev => ({ ...prev, isGenerating: false }));
        es.close();
        setEventSource(null);
      });

      setEventSource(es);
    },
    [eventSource]
  );

  const updateStatus = useCallback(
    (promptType: string, modelId: string, status: GenerationStatus) => {
      setState(prev => ({
        ...prev,
        statuses: {
          ...prev.statuses,
          [promptType]: {
            ...prev.statuses[promptType],
            [modelId]: status
          }
        }
      }));
    },
    []
  );

  const updateResult = useCallback(
    (promptType: string, modelId: string, result: GenerationResult) => {
      setState(prev => ({
        ...prev,
        results: {
          ...prev.results,
          [promptType]: {
            ...prev.results[promptType],
            [modelId]: result
          }
        }
      }));
    },
    []
  );

  const cancelGeneration = useCallback(() => {
    if (eventSource) {
      eventSource.close();
      setEventSource(null);
    }
    setState(prev => ({ ...prev, isGenerating: false }));
  }, [eventSource]);

  const clearResults = useCallback(() => {
    setState({
      statuses: {},
      results: {},
      totalGenerations: 0,
      completedGenerations: 0,
      failedGenerations: 0,
      isGenerating: false,
      generationId: null
    });
  }, []);

  const value: GenerationContextType = {
    ...state,
    startGeneration,
    updateStatus,
    updateResult,
    cancelGeneration,
    clearResults
  };

  return <GenerationContext.Provider value={value}>{children}</GenerationContext.Provider>;
}

export function useGeneration() {
  const context = useContext(GenerationContext);
  if (context === undefined) {
    throw new Error('useGeneration must be used within a GenerationProvider');
  }
  return context;
}
```

### Frontend Generation Panel Component

[Source: architecture.md - Source Tree and Module Organization - Frontend Module Structure]

**File: frontend/src/components/config/GenerationPanel.tsx** (NEW)

Complete generation UI with status indicators:

```typescript
'use client';

import { Card, Button, Alert, ProgressBar, Badge, Row, Col } from 'react-bootstrap';
import { useGeneration } from '@/contexts/GenerationContext';
import { useModel } from '@/contexts/ModelContext';
import { usePrompt } from '@/contexts/PromptContext';
import { useData } from '@/contexts/DataContext';

interface GenerationPanelProps {
  triggerId: string;
  stockId: string;
}

const STATUS_COLORS = {
  pending: 'secondary',
  generating: 'primary',
  complete: 'success',
  failed: 'danger'
} as const;

const STATUS_ICONS = {
  pending: '‚è∏Ô∏è',
  generating: '‚è≥',
  complete: '‚úÖ',
  failed: '‚ùå'
} as const;

export default function GenerationPanel({ triggerId, stockId }: GenerationPanelProps) {
  const {
    statuses,
    results,
    totalGenerations,
    completedGenerations,
    failedGenerations,
    isGenerating,
    startGeneration,
    cancelGeneration
  } = useGeneration();

  const { selectedModels, modelSettings } = useModel();
  const { checkedTypes, prompts } = usePrompt();
  const { structuredData } = useData();

  const handleGenerate = async () => {
    if (selectedModels.size === 0) {
      alert('Please select at least one model');
      return;
    }

    if (checkedTypes.size === 0) {
      alert('Please check at least one prompt type');
      return;
    }

    // Validate prompts exist for checked types
    for (const type of Array.from(checkedTypes)) {
      if (!prompts[type] || !prompts[type].content.trim()) {
        alert(`Please enter a prompt for ${type} type`);
        return;
      }
    }

    // Prepare prompts object
    const promptsObject: Record<string, string> = {};
    checkedTypes.forEach(type => {
      promptsObject[type] = prompts[type].content;
    });

    await startGeneration(
      triggerId,
      stockId,
      Array.from(selectedModels),
      Array.from(checkedTypes),
      promptsObject,
      structuredData,
      modelSettings
    );
  };

  const progressPercentage = totalGenerations > 0
    ? ((completedGenerations + failedGenerations) / totalGenerations) * 100
    : 0;

  const canGenerate = selectedModels.size > 0 && checkedTypes.size > 0 && !isGenerating;

  return (
    <Card className="mt-4">
      <Card.Header>
        <h5 className="mb-0">Generate News</h5>
      </Card.Header>

      <Card.Body>
        {/* Generation Button */}
        <div className="d-flex justify-content-between align-items-center mb-3">
          <Button
            variant="primary"
            size="lg"
            onClick={handleGenerate}
            disabled={!canGenerate}
          >
            {isGenerating ? (
              <>
                <span className="spinner-border spinner-border-sm me-2" />
                Generating...
              </>
            ) : (
              'Generate News'
            )}
          </Button>

          {isGenerating && (
            <Button variant="outline-danger" onClick={cancelGeneration}>
              Cancel
            </Button>
          )}
        </div>

        {/* Generation Info */}
        {selectedModels.size > 0 && checkedTypes.size > 0 && (
          <Alert variant="info" className="mb-3">
            <strong>Will generate:</strong> {selectedModels.size} models √ó {checkedTypes.size} prompt types = {selectedModels.size * checkedTypes.size} total generations
          </Alert>
        )}

        {/* Progress Bar */}
        {isGenerating && (
          <div className="mb-4">
            <div className="d-flex justify-content-between mb-2">
              <span>
                <strong>Progress:</strong> {completedGenerations + failedGenerations} of {totalGenerations} complete
              </span>
              <span className="text-muted">
                {completedGenerations} succeeded, {failedGenerations} failed
              </span>
            </div>
            <ProgressBar
              now={progressPercentage}
              label={`${Math.round(progressPercentage)}%`}
              variant={failedGenerations > 0 ? 'warning' : 'success'}
            />
          </div>
        )}

        {/* Status Display - Grouped by Prompt Type */}
        {Object.keys(statuses).length > 0 && (
          <div className="mt-4">
            <h6 className="mb-3">Generation Status</h6>

            {Object.entries(statuses).map(([promptType, modelStatuses]) => (
              <Card key={promptType} className="mb-3">
                <Card.Header className="bg-light">
                  <strong>
                    {promptType === 'paid' && 'üí∞ Paid'}
                    {promptType === 'unpaid' && 'üÜì Unpaid'}
                    {promptType === 'crawler' && 'üï∑Ô∏è Crawler'}
                  </strong>
                </Card.Header>
                <Card.Body>
                  <Row>
                    {Object.entries(modelStatuses).map(([modelId, status]) => {
                      const result = results[promptType]?.[modelId];

                      return (
                        <Col key={modelId} md={4} className="mb-2">
                          <div className="p-3 border rounded">
                            <div className="d-flex justify-content-between align-items-center mb-2">
                              <strong>{modelId}</strong>
                              <Badge bg={STATUS_COLORS[status]}>
                                {STATUS_ICONS[status]} {status.toUpperCase()}
                              </Badge>
                            </div>

                            {/* Error Message */}
                            {status === 'failed' && result?.error && (
                              <Alert variant="danger" className="mb-0 mt-2 small">
                                {result.error}
                              </Alert>
                            )}

                            {/* Success Summary */}
                            {status === 'complete' && result && (
                              <div className="small text-muted mt-2">
                                <div>üéØ Tokens: {result.token_count}</div>
                                <div>‚è±Ô∏è Time: {result.latency}s</div>
                                <div>üí∞ Cost: ${result.cost?.toFixed(4)}</div>
                              </div>
                            )}
                          </div>
                        </Col>
                      );
                    })}
                  </Row>
                </Card.Body>
              </Card>
            ))}
          </div>
        )}
      </Card.Body>
    </Card>
  );
}
```

### Implementation Notes

[Source: PRD - Epic 4: Multi-Model Generation & Testing]

**Important Design Decisions**:

1. **Parallel Execution**: asyncio.gather with return_exceptions=True ensures one failure doesn't block others. All generations run concurrently.

2. **Error Isolation**: Each (model √ó type) combination is independent. If GPT-4 for paid fails, Claude for paid and all unpaid generations continue unaffected.

3. **Real-Time Updates**: Server-Sent Events (SSE) provide real-time status updates without polling overhead. Falls back to polling if SSE not supported.

4. **Timeout Per Model**: 30 seconds default per generation. Configurable via environment variable. Prevents hung requests from blocking workflow.

5. **Status Lifecycle**: Pending (gray) ‚Üí Generating (blue) ‚Üí Complete (green) OR Failed (red). Clear visual feedback at each stage.

6. **Progress Tracking**: "X of Y complete" shows overall progress. Separate counts for succeeded vs failed.

7. **Session History**: Generation results stored in MongoDB with 30-day retention for session-level navigation.

8. **Grouped Display**: Results organized by prompt type first, then models within each type. Matches user mental model of comparing models per audience.

### Testing

[Source: PRD - Technical Assumptions - Testing Requirements]

**Testing Strategy for Story 4.3**:

**Unit Tests** (backend/tests/test_generation_service.py):

```python
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
from app.services.generation_service import GenerationService
import asyncio

@pytest.mark.asyncio
async def test_generate_parallel_success():
    """Test parallel generation with all successes"""
    service = GenerationService(timeout=30.0)

    # Mock provider results
    mock_result = {
        "generated_text": "Test output",
        "token_count": 100,
        "prompt_tokens": 50,
        "completion_tokens": 50,
        "model_name": "gpt-4",
        "latency": 2.5,
        "cost": 0.05
    }

    with patch.object(service, '_generate_single', new=AsyncMock(return_value=mock_result)):
        result = await service.generate_parallel(
            trigger_id="trigger_001",
            stock_id="AAPL",
            selected_models=["gpt-4", "claude-3-sonnet"],
            prompt_types=["paid", "unpaid"],
            prompts={
                "paid": "Paid prompt",
                "unpaid": "Unpaid prompt"
            },
            structured_data={},
            model_settings={"temperature": 0.7, "max_tokens": 500}
        )

        # Verify results structure
        assert "generation_id" in result
        assert "results" in result
        assert "metadata" in result

        # Verify all combinations present
        assert "paid" in result["results"]
        assert "unpaid" in result["results"]
        assert "gpt-4" in result["results"]["paid"]
        assert "claude-3-sonnet" in result["results"]["paid"]

        # Verify metadata
        assert result["metadata"]["total_generations"] == 4  # 2 models √ó 2 types
        assert result["metadata"]["completed"] == 4
        assert result["metadata"]["failed"] == 0

@pytest.mark.asyncio
async def test_generate_parallel_with_failures():
    """Test parallel generation with some failures"""
    service = GenerationService(timeout=30.0)

    # Mock one success, one failure
    async def mock_generate(model_id, prompt, model_settings, prompt_type, status_callback):
        if model_id == "gpt-4":
            return {
                "generated_text": "Success",
                "token_count": 100,
                "cost": 0.05,
                "latency": 2.0
            }
        else:
            raise Exception("API Error")

    with patch.object(service, '_generate_single', new=mock_generate):
        result = await service.generate_parallel(
            trigger_id="trigger_001",
            stock_id="AAPL",
            selected_models=["gpt-4", "claude-3-sonnet"],
            prompt_types=["paid"],
            prompts={"paid": "Test"},
            structured_data={},
            model_settings={}
        )

        # Verify GPT-4 succeeded
        assert result["results"]["paid"]["gpt-4"]["status"] == "complete"

        # Verify Claude failed
        assert result["results"]["paid"]["claude-3-sonnet"]["status"] == "failed"
        assert "error" in result["results"]["paid"]["claude-3-sonnet"]

        # Verify metadata
        assert result["metadata"]["completed"] == 1
        assert result["metadata"]["failed"] == 1

@pytest.mark.asyncio
async def test_generate_single_timeout():
    """Test timeout handling for individual generation"""
    service = GenerationService(timeout=1.0)  # 1 second timeout

    # Mock slow provider
    async def slow_generate(*args, **kwargs):
        await asyncio.sleep(2.0)  # Longer than timeout
        return {"generated_text": "Should not reach"}

    with patch.object(service.provider_registry, 'get_provider') as mock_get_provider:
        mock_provider = MagicMock()
        mock_provider.generate = slow_generate
        mock_get_provider.return_value = mock_provider

        with pytest.raises(TimeoutError):
            await service._generate_single(
                model_id="gpt-4",
                prompt="Test",
                model_settings={},
                prompt_type="paid"
            )

@pytest.mark.asyncio
async def test_status_callback():
    """Test status callback invocation"""
    service = GenerationService()
    status_updates = []

    async def status_callback(update):
        status_updates.append(update)

    mock_result = {"generated_text": "Test", "token_count": 100, "cost": 0.05, "latency": 1.0}

    with patch.object(service, '_generate_single', new=AsyncMock(return_value=mock_result)):
        await service.generate_parallel(
            trigger_id="trigger_001",
            stock_id="AAPL",
            selected_models=["gpt-4"],
            prompt_types=["paid"],
            prompts={"paid": "Test"},
            structured_data={},
            model_settings={},
            status_callback=status_callback
        )

        # Verify status updates sent
        assert len(status_updates) > 0
        assert any(u["status"] == "complete" for u in status_updates)
```

**Integration Tests** (backend/tests/integration/test_generation_endpoint.py):

```python
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_generation_endpoint_sse(client: AsyncClient, db):
    """Test generation endpoint with SSE"""
    # Create configuration
    await db.configurations.insert_one({
        "_id": "config_001",
        "trigger_name": "earnings_report",
        "model_config": {
            "selected_models": ["gpt-4"],
            "temperature": 0.7,
            "max_tokens": 500
        }
    })

    # Mock LLM provider
    with patch('app.services.generation_service.ProviderRegistry.get_provider') as mock_get:
        mock_provider = AsyncMock()
        mock_provider.generate.return_value = {
            "generated_text": "Test output",
            "token_count": 100,
            "cost": 0.05,
            "latency": 2.0
        }
        mock_get.return_value = mock_provider

        # Make SSE request
        async with client.stream(
            "POST",
            "/api/news/generate",
            json={
                "trigger_name": "earnings_report",
                "stockid": "AAPL",
                "data_mode": "OLD_NEW",
                "sections": ["headline", "body"],
                "section_order": ["headline", "body"],
                "selected_models": ["gpt-4"],
                "prompt_types": ["paid"],
                "prompts": {"paid": "Test prompt"},
                "structured_data": {},
                "model_settings": {"temperature": 0.7, "max_tokens": 500}
            }
        ) as response:
            assert response.status_code == 200
            assert response.headers["content-type"] == "text/event-stream"

            events = []
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    events.append(line)

            # Verify events received
            assert len(events) > 0

@pytest.mark.asyncio
async def test_generation_history(client: AsyncClient, db):
    """Test generation history retrieval"""
    # Insert generation history
    await db.generation_history.insert_one({
        "generation_id": "gen_001",
        "trigger_name": "earnings_report",
        "stockid": "AAPL",
        "data_mode": "OLD_NEW",
        "sections": ["headline", "body"],
        "section_order": ["headline", "body"],
        "timestamp": "2025-10-29T12:00:00",
        "prompt_types": ["paid"],
        "results": {"paid": {"gpt-4": {"generated_text": "Test"}}}
    })

    response = await client.get("/api/news/generation-history?trigger_name=earnings_report")

    assert response.status_code == 200
    data = response.json()
    assert "history" in data
    assert len(data["history"]) == 1
    assert data["history"][0]["generation_id"] == "gen_001"
```

**Frontend Tests** (frontend/__tests__/components/config/GenerationPanel.test.tsx):

```typescript
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import GenerationPanel from '@/components/config/GenerationPanel';
import { GenerationProvider } from '@/contexts/GenerationContext';
import { ModelProvider } from '@/contexts/ModelContext';
import { PromptProvider } from '@/contexts/PromptContext';
import { DataProvider } from '@/contexts/DataContext';

describe('GenerationPanel Component', () => {
  const renderWithContext = (component: React.ReactElement) => {
    return render(
      <DataProvider>
        <PromptProvider>
          <ModelProvider>
            <GenerationProvider>
              {component}
            </GenerationProvider>
          </ModelProvider>
        </PromptProvider>
      </DataProvider>
    );
  };

  test('renders generate button', () => {
    renderWithContext(<GenerationPanel triggerId="trigger_001" stockId="AAPL" />);

    expect(screen.getByText('Generate News')).toBeInTheDocument();
  });

  test('disables generate button when no models selected', () => {
    renderWithContext(<GenerationPanel triggerId="trigger_001" stockId="AAPL" />);

    const button = screen.getByText('Generate News');
    expect(button).toBeDisabled();
  });

  test('displays generation progress', async () => {
    renderWithContext(<GenerationPanel triggerId="trigger_001" stockId="AAPL" />);

    // Mock generation start
    // (Implementation depends on context setup)

    await waitFor(() => {
      expect(screen.getByText(/Progress:/)).toBeInTheDocument();
    });
  });

  test('displays status badges for each model', async () => {
    renderWithContext(<GenerationPanel triggerId="trigger_001" stockId="AAPL" />);

    // Mock generation with status updates
    // (Implementation depends on context setup)

    await waitFor(() => {
      expect(screen.getByText(/GENERATING/)).toBeInTheDocument();
    });
  });

  test('displays error messages for failed generations', async () => {
    renderWithContext(<GenerationPanel triggerId="trigger_001" stockId="AAPL" />);

    // Mock generation with failure
    // (Implementation depends on context setup)

    await waitFor(() => {
      expect(screen.getByText(/API Error/)).toBeInTheDocument();
    });
  });
});
```

**Manual Verification Checklist**:
1. "Generate News" button triggers parallel generation for all (model √ó type) combinations
2. Real-time status updates display for each combination (Pending ‚Üí Generating ‚Üí Complete/Failed)
3. Progress bar shows accurate percentage: (completed + failed) / total
4. Count display: "Generating 4 of 6 complete" updates in real-time
5. Status grouped by prompt type (üí∞ Paid, üÜì Unpaid, üï∑Ô∏è Crawler)
6. Models displayed side-by-side within each type group
7. Failed generations show error message without blocking others
8. Timeout triggers after 30 seconds per model
9. Generation results stored in MongoDB generation_history
10. Generation history accessible via API endpoint
11. SSE connection established and events streamed correctly
12. Cancel button stops generation and closes SSE connection
13. Page remains responsive during generation
14. No console errors or warnings
15. Cost, tokens, and latency displayed for successful generations

## News CMS Workflow Updates

### Overview

This story has been updated to align with the News CMS Workflow Feature requirements. The key changes reflect a shift from model-centric to prompt-type-centric generation.

### Key Changes

**1. Generation Per Prompt Type (Not Per Model)**

The workflow now generates news **PER PROMPT TYPE** rather than per model:
- Users select which prompt types to generate: Paid (üí∞), Unpaid (üÜì), Crawler (üï∑Ô∏è)
- Each prompt type can be generated using multiple models in parallel
- Results are grouped and displayed by prompt type first, then by model within each type
- This aligns with the content manager's mental model of comparing different audience segments

**2. Adaptive Routing via isActive Flag**

The NewsGenerationService uses the `trigger_prompts.isActive` flag for backward compatibility:
- If `isActive = false`: Routes to legacy 3-prompt method (`generate_news_og.py`)
- If `isActive = true`: Uses new merged data approach with single HTML prompt per type
- This allows gradual migration from legacy to new system without breaking existing triggers

**3. Status Grouped by Prompt Type for MVP**

Status indicators and progress tracking are organized by prompt type:
- Status display shows prompt type sections: [üí∞ Paid] [üÜì Unpaid] [üï∑Ô∏è Crawler]
- Within each section, model-specific status badges are shown
- Progress bar tracks completion across all prompt types
- Error messages are isolated per prompt type to prevent blocking other generations

**4. Results with Colored Headers**

Results are displayed with color-coded headers for visual distinction:
- **Blue**: Paid subscriber content (üí∞)
- **Green**: Unpaid/free tier content (üÜì)
- **Orange**: Crawler/search engine content (üï∑Ô∏è)

This visual hierarchy helps content managers quickly identify and compare content for different audience segments.

**5. Request Parameters**

The API endpoint `POST /api/news/generate` accepts the following parameters:
- `trigger_name`: Name of the trigger being executed
- `stockid`: Stock identifier (e.g., "AAPL")
- `data_mode`: Data merge mode ("OLD", "NEW", or "OLD_NEW")
- `sections`: List of content sections to generate (e.g., ["headline", "body", "analysis"])
- `section_order`: Ordered list defining section sequence
- `prompt_types`: List of checked prompt types to generate (e.g., ["paid", "unpaid"])
- `selected_models`: List of models to use for generation
- `prompts`: Dictionary mapping prompt type to prompt content
- `structured_data`: Merged data structure from Story 2.3
- `model_settings`: Temperature, max_tokens, and other LLM parameters

### Implementation Impact

**Backend Changes**:
- Endpoint changed from `POST /api/triggers/:id/generate` to `POST /api/news/generate`
- Request model updated to include trigger_name, stockid, data_mode, sections, section_order
- Generation service orchestrates parallel calls per prompt type
- Results stored with prompt type context in generation_history collection

**Frontend Changes**:
- GenerationPanel displays results grouped by prompt type
- Color-coded headers for visual distinction
- Progress tracking per prompt type
- Status indicators scoped to prompt type + model combinations

**Database Changes**:
- generation_history collection includes trigger_name, data_mode, sections fields
- Results structure: `{prompt_type: {model_id: {...}}}`

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-29 | 1.0 | Initial story created from Epic 4 | Sarah (PO) |
| 2025-10-29 | 1.1 | Enriched with full architectural context and testing standards | Bob (SM) |
| 2025-10-30 | 2.0 | Aligned with News CMS Workflow Feature requirements: Changed endpoint to POST /api/news/generate, added trigger_name/stockid/data_mode parameters, documented prompt-type-centric generation approach, updated prerequisites to reference v2.0 stories | Bob (SM) |

## Dev Agent Record

*To be populated during implementation*

### Agent Model Used

*To be filled by dev agent*

### Debug Log References

*To be filled by dev agent*

### Completion Notes List

*To be filled by dev agent*

### File List

*To be filled by dev agent*

## QA Results

*To be filled by QA agent*
