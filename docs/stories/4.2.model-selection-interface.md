# Story 4.2: Model Selection Interface (Shared Across All Prompt Types)

## Status

Draft

## Story

**As a** content manager,
**I want** to select multiple LLM models for parallel testing with adjustable settings that will be used for all checked prompt types,
**so that** I can compare different models' outputs across different audience types before choosing the best configuration.

## Acceptance Criteria

1. "Model Selection" panel displays available models grouped by provider (OpenAI, Anthropic, Google) with header label "(Used for All Types)"
2. Each model shows checkbox for selection, model name, and brief description
3. Selected models display additional settings: temperature slider (0.0-1.0), max tokens input
4. Cost estimate displayed per model based on average prompt size and configured max tokens
5. Total estimated cost calculation: (models √ó checked prompt types) shown prominently with breakdown
6. Example: 2 models √ó 3 prompt types (paid, unpaid, crawler) = 6 generations total
7. Default settings pre-configured (temperature=0.7, max_tokens=500) but user-adjustable
8. Meets FR17 and FR18: model selection with settings and cost estimates
9. At least one model must be selected to enable "Generate News" button
10. Model selection and settings saved to Configuration for reuse across all prompt types
11. Help tooltips explain temperature and max tokens parameters for non-technical users
12. Visual indicator shows "Will generate for: Paid, Unpaid, Crawler" based on checked types

## Tasks / Subtasks

- [ ] Task 1: Create ModelContext provider for model selection state (AC: 9, 10)
  - [ ] Create frontend/src/contexts/ModelContext.tsx
  - [ ] State management for selected models, temperature, max_tokens
  - [ ] Save/load model configuration from Configuration
  - [ ] Track cost estimates per model
- [ ] Task 2: Create ModelSelection component (AC: 1, 2, 3, 11)
  - [ ] Create frontend/src/components/config/ModelSelection.tsx
  - [ ] Display models grouped by provider
  - [ ] Model checkboxes with selection state
  - [ ] Temperature slider (0.0-1.0) with tooltip
  - [ ] Max tokens input field with tooltip
  - [ ] Default settings (0.7 temperature, 500 tokens)
- [ ] Task 3: Implement cost estimation logic (AC: 4, 5, 6, 12)
  - [ ] Calculate cost per model: tokens √ó pricing
  - [ ] Total cost: (models √ó checked prompt types)
  - [ ] Display breakdown: "2 models √ó 3 types = 6 generations"
  - [ ] Visual indicator for checked prompt types
  - [ ] Update estimates in real-time as selection changes
- [ ] Task 4: Create backend endpoint for model configuration (AC: 10)
  - [ ] Create POST /api/triggers/:id/config/models endpoint
  - [ ] Update model_config field in Configuration document
  - [ ] Return updated configuration
- [ ] Task 5: Integrate with Generation Context (AC: 9)
  - [ ] Disable "Generate News" button if no models selected
  - [ ] Pass selected models to generation endpoint (Story 4.3)
  - [ ] Display selected models in generation UI
- [ ] Task 6: Write unit and integration tests
  - [ ] Test ModelContext state management
  - [ ] Test cost calculation logic
  - [ ] Test model selection/deselection
  - [ ] Test temperature and max_tokens updates
  - [ ] Test backend endpoint

## Dev Notes

### Prerequisites from Previous Stories

[Source: Story 4.1 - Completion]

Before starting this story, ensure Story 4.1 is complete:
- LLM provider abstraction layer implemented
- Provider registry lists available models
- Cost calculation logic in each provider
- API keys configured for all providers

**Epic 3 Dependencies**:
- PromptEditor component displays checked prompt types (Story 3.2)
- TriggerContextBar component shows prompt type checkboxes (Story 2.1)
- Configuration model supports model_config field (Story 1.2)

### Project Structure & File Locations

[Source: architecture.md - Source Tree and Module Organization - Frontend Module Structure]

**Frontend files to create/modify** in `frontend/src/`:
```
frontend/src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ ModelSelection.tsx          # Model selection panel (NEW)
‚îÇ       ‚îú‚îÄ‚îÄ ModelCard.tsx               # Individual model card (NEW)
‚îÇ       ‚îú‚îÄ‚îÄ CostEstimator.tsx           # Cost estimation display (NEW)
‚îÇ       ‚îî‚îÄ‚îÄ ConfigurationWorkspace.tsx  # Integrate ModelSelection (MODIFY)
‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îú‚îÄ‚îÄ ModelContext.tsx                # Model selection state (NEW)
‚îÇ   ‚îî‚îÄ‚îÄ PromptContext.tsx               # Existing prompt context
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ model.ts                        # TypeScript types for models (NEW)
```

**Backend files to modify** in `backend/app/`:
```
backend/app/
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îî‚îÄ‚îÄ configuration.py                # Add model configuration endpoint (MODIFY)
‚îî‚îÄ‚îÄ services/
    ‚îî‚îÄ‚îÄ configuration_service.py        # Add update_models method (MODIFY)
```

### Technology Stack Details

[Source: architecture.md - High Level Architecture - Planned Tech Stack]

**Frontend Stack**:
- **React-Bootstrap**: For cards, checkboxes, form controls
- **React Context API**: ModelContext for model selection state
- **TypeScript**: Type safety for model operations
- **React Hooks**: useState, useEffect, useCallback

**Backend Stack**:
- **FastAPI**: POST /api/triggers/:id/config/models endpoint
- **Pydantic**: Model configuration validation
- **MongoDB**: Store model_config in Configuration document

### Model Selection Architecture

[Source: PRD - Epic 4: Multi-Model Generation & Testing]

**Shared Model Selection Pattern**:
- **Applies to All Prompt Types**: Model selection is SHARED across paid, unpaid, crawler
- **Cost Calculation**: (selected models) √ó (checked prompt types) = total generations
- **Example**: 2 models √ó 3 types = 6 parallel API calls
- **Visual Indicator**: "Will generate for: Paid, Unpaid, Crawler" based on checked types
- **Default State**: No models selected initially; user must select at least one

**Why Shared Selection?**:
- Simplifies UI: Single model selection panel
- Consistent comparison: Same models test all prompt types
- Cost transparency: Clear total cost calculation
- Efficient testing: Parallel generation for all combinations

### React Context for Model Management

[Source: architecture.md - Source Tree and Module Organization - React Context API]

**File: frontend/src/contexts/ModelContext.tsx** (NEW)

Complete model selection state management:

```typescript
'use client';

import React, { createContext, useContext, useState, useEffect, ReactNode, useCallback } from 'react';
import { usePrompt } from './PromptContext';

interface ModelSettings {
  temperature: number;
  max_tokens: number;
}

interface ModelContextType {
  selectedModels: Set<string>;
  modelSettings: ModelSettings;
  estimatedCost: number;
  costBreakdown: {
    modelsCount: number;
    typesCount: number;
    totalGenerations: number;
    costPerModel: Record<string, number>;
  };
  toggleModel: (modelId: string) => void;
  setTemperature: (temp: number) => void;
  setMaxTokens: (tokens: number) => void;
  saveModelConfig: (triggerId: string) => Promise<void>;
  loadModelConfig: (triggerId: string) => Promise<void>;
  isSaving: boolean;
  saveError: string | null;
}

const ModelContext = createContext<ModelContextType | undefined>(undefined);

// Model pricing data (from Story 4.1 providers)
const MODEL_PRICING: Record<string, { prompt: number; completion: number }> = {
  "gpt-4": { prompt: 0.03, completion: 0.06 },
  "gpt-4-turbo": { prompt: 0.01, completion: 0.03 },
  "gpt-3.5-turbo": { prompt: 0.0005, completion: 0.0015 },
  "claude-3-opus": { prompt: 15, completion: 75 },
  "claude-3-sonnet": { prompt: 3, completion: 15 },
  "claude-3-haiku": { prompt: 0.25, completion: 1.25 },
  "gemini-pro": { prompt: 0.25, completion: 0.5 },
  "gemini-1.5-pro": { prompt: 1.25, completion: 5 }
};

export function ModelProvider({ children }: { children: ReactNode }) {
  const [selectedModels, setSelectedModels] = useState<Set<string>>(new Set());
  const [modelSettings, setModelSettings] = useState<ModelSettings>({
    temperature: 0.7,
    max_tokens: 500
  });
  const [isSaving, setIsSaving] = useState(false);
  const [saveError, setSaveError] = useState<string | null>(null);

  const { checkedTypes, prompts } = usePrompt();

  // Calculate estimated cost
  const calculateCost = useCallback(() => {
    if (selectedModels.size === 0 || checkedTypes.size === 0) {
      return {
        total: 0,
        breakdown: {
          modelsCount: 0,
          typesCount: 0,
          totalGenerations: 0,
          costPerModel: {}
        }
      };
    }

    // Estimate average prompt length from current prompts
    let avgPromptLength = 0;
    checkedTypes.forEach(type => {
      avgPromptLength += prompts[type]?.content.length || 0;
    });
    avgPromptLength = avgPromptLength / checkedTypes.size;

    // Estimate tokens (rough: 4 chars ‚âà 1 token)
    const estimatedPromptTokens = Math.ceil(avgPromptLength / 4);
    const estimatedCompletionTokens = modelSettings.max_tokens;

    // Calculate cost per model
    const costPerModel: Record<string, number> = {};
    let totalCost = 0;

    selectedModels.forEach(modelId => {
      const pricing = MODEL_PRICING[modelId] || { prompt: 0.03, completion: 0.06 };

      // Cost calculation depends on pricing units (some are per 1K, some per 1M)
      let modelCost;
      if (modelId.startsWith("claude")) {
        // Anthropic pricing is per 1M tokens
        modelCost =
          (estimatedPromptTokens / 1_000_000) * pricing.prompt +
          (estimatedCompletionTokens / 1_000_000) * pricing.completion;
      } else {
        // OpenAI and Google pricing is per 1K tokens
        modelCost =
          (estimatedPromptTokens / 1000) * pricing.prompt +
          (estimatedCompletionTokens / 1000) * pricing.completion;
      }

      // Multiply by number of prompt types
      const modelTotalCost = modelCost * checkedTypes.size;
      costPerModel[modelId] = modelTotalCost;
      totalCost += modelTotalCost;
    });

    return {
      total: totalCost,
      breakdown: {
        modelsCount: selectedModels.size,
        typesCount: checkedTypes.size,
        totalGenerations: selectedModels.size * checkedTypes.size,
        costPerModel
      }
    };
  }, [selectedModels, checkedTypes, prompts, modelSettings.max_tokens]);

  const { total: estimatedCost, breakdown: costBreakdown } = calculateCost();

  const toggleModel = (modelId: string) => {
    setSelectedModels(prev => {
      const newSet = new Set(prev);
      if (newSet.has(modelId)) {
        newSet.delete(modelId);
      } else {
        newSet.add(modelId);
      }
      return newSet;
    });
  };

  const setTemperature = (temp: number) => {
    setModelSettings(prev => ({ ...prev, temperature: temp }));
  };

  const setMaxTokens = (tokens: number) => {
    setModelSettings(prev => ({ ...prev, max_tokens: tokens }));
  };

  const saveModelConfig = async (triggerId: string) => {
    try {
      setIsSaving(true);
      setSaveError(null);

      const response = await fetch(`/api/triggers/${triggerId}/config/models`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          selected_models: Array.from(selectedModels),
          temperature: modelSettings.temperature,
          max_tokens: modelSettings.max_tokens
        })
      });

      if (!response.ok) {
        throw new Error('Failed to save model configuration');
      }

    } catch (err) {
      setSaveError(err instanceof Error ? err.message : 'Failed to save model configuration');
      throw err;
    } finally {
      setIsSaving(false);
    }
  };

  const loadModelConfig = async (triggerId: string) => {
    try {
      const response = await fetch(`/api/triggers/${triggerId}/config`);

      if (!response.ok) {
        throw new Error('Failed to load configuration');
      }

      const config = await response.json();

      if (config.model_config) {
        setSelectedModels(new Set(config.model_config.selected_models || []));
        setModelSettings({
          temperature: config.model_config.temperature || 0.7,
          max_tokens: config.model_config.max_tokens || 500
        });
      }
    } catch (err) {
      setSaveError(err instanceof Error ? err.message : 'Failed to load model configuration');
    }
  };

  const value: ModelContextType = {
    selectedModels,
    modelSettings,
    estimatedCost,
    costBreakdown,
    toggleModel,
    setTemperature,
    setMaxTokens,
    saveModelConfig,
    loadModelConfig,
    isSaving,
    saveError
  };

  return <ModelContext.Provider value={value}>{children}</ModelContext.Provider>;
}

export function useModel() {
  const context = useContext(ModelContext);
  if (context === undefined) {
    throw new Error('useModel must be used within a ModelProvider');
  }
  return context;
}
```

### Frontend Component Specifications

[Source: architecture.md - Source Tree and Module Organization - Frontend Module Structure]

**File: frontend/src/components/config/ModelSelection.tsx** (NEW)

Complete model selection panel with cost estimation:

```typescript
'use client';

import { useEffect } from 'react';
import { Card, Alert, Badge, Row, Col } from 'react-bootstrap';
import { useModel } from '@/contexts/ModelContext';
import { usePrompt } from '@/contexts/PromptContext';
import ModelCard from './ModelCard';
import CostEstimator from './CostEstimator';

interface ModelSelectionProps {
  triggerId: string;
}

// Available models by provider (from Story 4.1 registry)
const AVAILABLE_MODELS = {
  "OpenAI": [
    { id: "gpt-4", name: "GPT-4", description: "Most capable model, best quality" },
    { id: "gpt-4-turbo", name: "GPT-4 Turbo", description: "Faster GPT-4 with lower cost" },
    { id: "gpt-3.5-turbo", name: "GPT-3.5 Turbo", description: "Fast and economical" }
  ],
  "Anthropic": [
    { id: "claude-3-opus", name: "Claude 3 Opus", description: "Highest performance" },
    { id: "claude-3-sonnet", name: "Claude 3 Sonnet", description: "Balanced performance and speed" },
    { id: "claude-3-haiku", name: "Claude 3 Haiku", description: "Fastest and most compact" }
  ],
  "Google": [
    { id: "gemini-pro", name: "Gemini Pro", description: "Google's advanced model" },
    { id: "gemini-1.5-pro", name: "Gemini 1.5 Pro", description: "Latest Gemini model" }
  ]
};

export default function ModelSelection({ triggerId }: ModelSelectionProps) {
  const {
    selectedModels,
    modelSettings,
    toggleModel,
    setTemperature,
    setMaxTokens,
    loadModelConfig,
    saveError
  } = useModel();

  const { checkedTypes } = usePrompt();

  // Load model config on mount
  useEffect(() => {
    loadModelConfig(triggerId);
  }, [triggerId]);

  return (
    <Card className="mt-4">
      <Card.Header className="d-flex justify-content-between align-items-center">
        <div>
          <h5 className="mb-0">
            Model Selection
            <Badge bg="info" className="ms-2">Used for All Types</Badge>
          </h5>
          <small className="text-muted">
            Select models to test across all checked prompt types
          </small>
        </div>
      </Card.Header>

      <Card.Body>
        {saveError && (
          <Alert variant="danger" className="mb-3" dismissible>
            {saveError}
          </Alert>
        )}

        {selectedModels.size === 0 && (
          <Alert variant="warning" className="mb-3">
            <i className="bi bi-exclamation-triangle me-2"></i>
            Please select at least one model to enable generation
          </Alert>
        )}

        {/* Model Settings */}
        <div className="mb-4 p-3 bg-light rounded">
          <h6 className="mb-3">Generation Settings</h6>
          <Row>
            <Col md={6}>
              <label className="form-label">
                Temperature: {modelSettings.temperature}
                <i
                  className="bi bi-info-circle ms-2 text-muted"
                  title="Controls randomness: 0 = deterministic, 1 = creative"
                ></i>
              </label>
              <input
                type="range"
                className="form-range"
                min="0"
                max="1"
                step="0.1"
                value={modelSettings.temperature}
                onChange={(e) => setTemperature(parseFloat(e.target.value))}
              />
            </Col>
            <Col md={6}>
              <label className="form-label">
                Max Tokens: {modelSettings.max_tokens}
                <i
                  className="bi bi-info-circle ms-2 text-muted"
                  title="Maximum length of generated text (approx 4 chars per token)"
                ></i>
              </label>
              <input
                type="number"
                className="form-control"
                min="50"
                max="4000"
                step="50"
                value={modelSettings.max_tokens}
                onChange={(e) => setMaxTokens(parseInt(e.target.value))}
              />
            </Col>
          </Row>
        </div>

        {/* Visual Indicator for Checked Types */}
        <div className="mb-3 p-2 bg-info-subtle rounded">
          <strong>Will generate for:</strong> {
            checkedTypes.size > 0
              ? Array.from(checkedTypes).map(type =>
                  type === 'paid' ? 'üí∞ Paid' :
                  type === 'unpaid' ? 'üÜì Unpaid' :
                  'üï∑Ô∏è Crawler'
                ).join(', ')
              : 'No prompt types checked'
          }
        </div>

        {/* Models Grouped by Provider */}
        {Object.entries(AVAILABLE_MODELS).map(([provider, models]) => (
          <div key={provider} className="mb-4">
            <h6 className="text-muted mb-3">{provider}</h6>
            <Row>
              {models.map(model => (
                <Col key={model.id} md={4} className="mb-3">
                  <ModelCard
                    model={model}
                    isSelected={selectedModels.has(model.id)}
                    onToggle={() => toggleModel(model.id)}
                  />
                </Col>
              ))}
            </Row>
          </div>
        ))}

        {/* Cost Estimator */}
        <CostEstimator />
      </Card.Body>
    </Card>
  );
}
```

**File: frontend/src/components/config/ModelCard.tsx** (NEW)

Individual model card component:

```typescript
'use client';

import { Card, Form } from 'react-bootstrap';

interface ModelCardProps {
  model: {
    id: string;
    name: string;
    description: string;
  };
  isSelected: boolean;
  onToggle: () => void;
}

export default function ModelCard({ model, isSelected, onToggle }: ModelCardProps) {
  return (
    <Card
      className={`h-100 cursor-pointer ${isSelected ? 'border-primary' : ''}`}
      onClick={onToggle}
      style={{ cursor: 'pointer' }}
    >
      <Card.Body>
        <Form.Check
          type="checkbox"
          checked={isSelected}
          onChange={onToggle}
          onClick={(e) => e.stopPropagation()}
          label={
            <div>
              <strong>{model.name}</strong>
              <br />
              <small className="text-muted">{model.description}</small>
            </div>
          }
        />
      </Card.Body>
    </Card>
  );
}
```

**File: frontend/src/components/config/CostEstimator.tsx** (NEW)

Cost estimation display component:

```typescript
'use client';

import { Alert, Table } from 'react-bootstrap';
import { useModel } from '@/contexts/ModelContext';

export default function CostEstimator() {
  const { estimatedCost, costBreakdown } = useModel();

  if (costBreakdown.totalGenerations === 0) {
    return null;
  }

  return (
    <Alert variant="success" className="mt-3">
      <h6 className="alert-heading">
        <i className="bi bi-calculator me-2"></i>
        Cost Estimate
      </h6>

      <div className="mb-2">
        <strong>Total Generations:</strong> {costBreakdown.modelsCount} models √ó {costBreakdown.typesCount} prompt types = <Badge bg="primary">{costBreakdown.totalGenerations} generations</Badge>
      </div>

      <div className="mb-3">
        <strong>Estimated Total Cost:</strong> <Badge bg="success">${estimatedCost.toFixed(4)}</Badge>
      </div>

      <details>
        <summary className="cursor-pointer text-muted">Cost Breakdown</summary>
        <Table size="sm" className="mt-2 mb-0">
          <thead>
            <tr>
              <th>Model</th>
              <th className="text-end">Cost per Type</th>
              <th className="text-end">Total Cost</th>
            </tr>
          </thead>
          <tbody>
            {Object.entries(costBreakdown.costPerModel).map(([modelId, cost]) => (
              <tr key={modelId}>
                <td>{modelId}</td>
                <td className="text-end">${(cost / costBreakdown.typesCount).toFixed(4)}</td>
                <td className="text-end">${cost.toFixed(4)}</td>
              </tr>
            ))}
          </tbody>
        </Table>
      </details>
    </Alert>
  );
}
```

### Backend API Implementation

[Source: architecture.md - Data Models and APIs - Epic 4 APIs]

**File: backend/app/routers/configuration.py** (UPDATE)

Add model configuration endpoint:

```python
from fastapi import APIRouter, HTTPException, Depends
from typing import List
from pydantic import BaseModel
from ..database import get_database
from ..services.configuration_service import ConfigurationService
import logging

router = APIRouter(prefix="/api/triggers/{trigger_id}/config", tags=["configuration"])
logger = logging.getLogger(__name__)

# ... (existing endpoints)

class ModelConfigRequest(BaseModel):
    selected_models: List[str]
    temperature: float
    max_tokens: int

@router.post("/models")
async def update_model_config(
    trigger_id: str,
    request: ModelConfigRequest,
    db = Depends(get_database)
):
    """
    Update model selection and settings.
    Model selection applies to ALL prompt types.
    """
    service = ConfigurationService(db)

    try:
        # Validate model IDs (must be in registry)
        valid_models = [
            "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo",
            "claude-3-opus", "claude-3-sonnet", "claude-3-haiku",
            "gemini-pro", "gemini-1.5-pro"
        ]

        for model_id in request.selected_models:
            if model_id not in valid_models:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid model ID: {model_id}"
                )

        # Validate temperature (0.0-1.0)
        if not 0.0 <= request.temperature <= 1.0:
            raise HTTPException(
                status_code=400,
                detail="Temperature must be between 0.0 and 1.0"
            )

        # Validate max_tokens (50-4000)
        if not 50 <= request.max_tokens <= 4000:
            raise HTTPException(
                status_code=400,
                detail="Max tokens must be between 50 and 4000"
            )

        config = await service.update_model_config(
            trigger_id,
            request.selected_models,
            request.temperature,
            request.max_tokens
        )

        return {
            "success": True,
            "configuration": config
        }
    except Exception as e:
        logger.error(f"Failed to update model config: {e}")
        raise HTTPException(status_code=400, detail=str(e))
```

**File: backend/app/services/configuration_service.py** (UPDATE)

Add update_model_config method:

```python
from datetime import datetime
from typing import List
from motor.motor_asyncio import AsyncIOMotorDatabase

class ConfigurationService:
    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db

    # ... (existing methods)

    async def update_model_config(
        self,
        trigger_id: str,
        selected_models: List[str],
        temperature: float,
        max_tokens: int
    ) -> Dict:
        """Update model configuration (applies to all prompt types)"""
        config = await self.get_configuration(trigger_id)

        # Update model_config
        update_data = {
            "$set": {
                "model_config.selected_models": selected_models,
                "model_config.temperature": temperature,
                "model_config.max_tokens": max_tokens,
                "updated_at": datetime.utcnow()
            }
        }

        await self.db.configurations.update_one(
            {"_id": config["_id"]},
            update_data
        )

        # Return updated configuration
        return await self.db.configurations.find_one({"_id": config["_id"]})
```

### Implementation Notes

[Source: PRD - Epic 4: Multi-Model Generation & Testing]

**Important Design Decisions**:

1. **Shared Model Selection**: Model selection applies to ALL checked prompt types. Simplifies UI and ensures consistent comparison.

2. **Cost Calculation**: (models √ó prompt types) = total generations. Real-time cost estimation based on estimated tokens.

3. **Default Settings**: Temperature = 0.7, Max Tokens = 500. Industry-standard defaults suitable for most use cases.

4. **Model Grouping**: Models grouped by provider (OpenAI, Anthropic, Google) for easy navigation.

5. **Visual Indicator**: "Will generate for: Paid, Unpaid, Crawler" shows which prompt types are active.

6. **Tooltips**: Help text explains temperature and max_tokens for non-technical users.

7. **Validation**: Backend validates model IDs, temperature range (0-1), max tokens range (50-4000).

### Testing

[Source: PRD - Technical Assumptions - Testing Requirements]

**Testing Strategy for Story 4.2**:

**Unit Tests** (frontend/__tests__/components/config/):

```typescript
// ModelSelection.test.tsx
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import ModelSelection from '@/components/config/ModelSelection';
import { ModelProvider } from '@/contexts/ModelContext';
import { PromptProvider } from '@/contexts/PromptContext';
import { DataProvider } from '@/contexts/DataContext';

describe('ModelSelection Component', () => {
  const renderWithContext = (component: React.ReactElement) => {
    return render(
      <DataProvider>
        <PromptProvider>
          <ModelProvider>
            {component}
          </ModelProvider>
        </PromptProvider>
      </DataProvider>
    );
  };

  test('renders model selection panel with header', () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    expect(screen.getByText('Model Selection')).toBeInTheDocument();
    expect(screen.getByText('Used for All Types')).toBeInTheDocument();
  });

  test('displays models grouped by provider', () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    expect(screen.getByText('OpenAI')).toBeInTheDocument();
    expect(screen.getByText('Anthropic')).toBeInTheDocument();
    expect(screen.getByText('Google')).toBeInTheDocument();
  });

  test('toggles model selection on checkbox click', async () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    const gpt4Checkbox = screen.getByLabelText(/GPT-4/);
    expect(gpt4Checkbox).not.toBeChecked();

    fireEvent.click(gpt4Checkbox);

    await waitFor(() => {
      expect(gpt4Checkbox).toBeChecked();
    });
  });

  test('updates temperature slider', () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    const slider = screen.getByLabelText(/Temperature/);
    expect(slider).toHaveValue('0.7');

    fireEvent.change(slider, { target: { value: '0.5' } });

    expect(slider).toHaveValue('0.5');
  });

  test('updates max tokens input', () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    const input = screen.getByLabelText(/Max Tokens/);
    expect(input).toHaveValue(500);

    fireEvent.change(input, { target: { value: '1000' } });

    expect(input).toHaveValue(1000);
  });

  test('displays warning when no models selected', () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    expect(screen.getByText(/Please select at least one model/)).toBeInTheDocument();
  });

  test('calculates cost estimate correctly', async () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    // Select 2 models
    fireEvent.click(screen.getByLabelText(/GPT-4/));
    fireEvent.click(screen.getByLabelText(/Claude 3 Sonnet/));

    await waitFor(() => {
      expect(screen.getByText(/Cost Estimate/)).toBeInTheDocument();
      expect(screen.getByText(/2 models/)).toBeInTheDocument();
    });
  });

  test('displays checked prompt types indicator', async () => {
    renderWithContext(<ModelSelection triggerId="trigger_001" />);

    // Assuming paid is checked by default
    expect(screen.getByText(/Will generate for:/)).toBeInTheDocument();
  });
});

// ModelContext.test.tsx
import { renderHook, act } from '@testing-library/react';
import { ModelProvider, useModel } from '@/contexts/ModelContext';

describe('ModelContext', () => {
  test('toggles model selection', () => {
    const wrapper = ({ children }) => <ModelProvider>{children}</ModelProvider>;
    const { result } = renderHook(() => useModel(), { wrapper });

    expect(result.current.selectedModels.size).toBe(0);

    act(() => {
      result.current.toggleModel('gpt-4');
    });

    expect(result.current.selectedModels.has('gpt-4')).toBe(true);

    act(() => {
      result.current.toggleModel('gpt-4');
    });

    expect(result.current.selectedModels.has('gpt-4')).toBe(false);
  });

  test('updates temperature', () => {
    const wrapper = ({ children }) => <ModelProvider>{children}</ModelProvider>;
    const { result } = renderHook(() => useModel(), { wrapper });

    expect(result.current.modelSettings.temperature).toBe(0.7);

    act(() => {
      result.current.setTemperature(0.5);
    });

    expect(result.current.modelSettings.temperature).toBe(0.5);
  });

  test('calculates cost estimate', () => {
    const wrapper = ({ children }) => <ModelProvider>{children}</ModelProvider>;
    const { result } = renderHook(() => useModel(), { wrapper });

    act(() => {
      result.current.toggleModel('gpt-4');
    });

    expect(result.current.estimatedCost).toBeGreaterThan(0);
  });
});
```

**Integration Tests** (backend/tests/test_configuration.py):

```python
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_update_model_config(client: AsyncClient, db):
    """Test POST /api/triggers/:id/config/models endpoint"""
    # Create configuration
    await db.configurations.insert_one({
        "_id": "config_001",
        "trigger_id": "trigger_001",
        "model_config": {}
    })

    response = await client.post(
        "/api/triggers/trigger_001/config/models",
        json={
            "selected_models": ["gpt-4", "claude-3-sonnet"],
            "temperature": 0.7,
            "max_tokens": 500
        }
    )

    assert response.status_code == 200
    data = response.json()
    assert data["success"] is True
    assert data["configuration"]["model_config"]["selected_models"] == ["gpt-4", "claude-3-sonnet"]
    assert data["configuration"]["model_config"]["temperature"] == 0.7
    assert data["configuration"]["model_config"]["max_tokens"] == 500

@pytest.mark.asyncio
async def test_model_config_validation(client: AsyncClient):
    """Test model config validation"""
    # Invalid model ID
    response = await client.post(
        "/api/triggers/trigger_001/config/models",
        json={
            "selected_models": ["invalid-model"],
            "temperature": 0.7,
            "max_tokens": 500
        }
    )
    assert response.status_code == 400
    assert "Invalid model ID" in response.json()["detail"]

    # Invalid temperature
    response = await client.post(
        "/api/triggers/trigger_001/config/models",
        json={
            "selected_models": ["gpt-4"],
            "temperature": 1.5,
            "max_tokens": 500
        }
    )
    assert response.status_code == 400
    assert "Temperature" in response.json()["detail"]

    # Invalid max_tokens
    response = await client.post(
        "/api/triggers/trigger_001/config/models",
        json={
            "selected_models": ["gpt-4"],
            "temperature": 0.7,
            "max_tokens": 5000
        }
    )
    assert response.status_code == 400
    assert "Max tokens" in response.json()["detail"]
```

**Manual Verification Checklist**:
1. Model selection panel displays with "Used for All Types" badge
2. Models grouped by provider (OpenAI, Anthropic, Google)
3. Checkboxes toggle model selection
4. Temperature slider adjusts from 0.0 to 1.0
5. Max tokens input accepts values from 50 to 4000
6. Default settings (0.7 temperature, 500 tokens) applied on load
7. Cost estimate displayed with breakdown: models √ó types
8. Visual indicator shows checked prompt types
9. Warning displayed when no models selected
10. Help tooltips explain temperature and max_tokens
11. Model configuration saved to MongoDB
12. Configuration persists on page reload
13. "Generate News" button disabled when no models selected

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-29 | 1.0 | Initial story created from Epic 4 | Sarah (PO) |
| 2025-10-29 | 1.1 | Enriched with full architectural context, complete frontend/backend implementations, and testing standards | Bob (SM) |

## Dev Agent Record

*To be populated during implementation*

### Agent Model Used

*To be filled by dev agent*

### Debug Log References

*To be filled by dev agent*

### Completion Notes List

*To be filled by dev agent*

### File List

*To be filled by dev agent*

## QA Results

*To be filled by QA agent*
