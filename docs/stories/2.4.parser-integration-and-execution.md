# Story 2.4: Structured Data Generation with generate_full_report.py

## Status

Draft

## Version

2.0 (MAJOR UPDATE - News CMS Workflow Feature Integration)

## Story

**As a** content manager,
**I want** to generate structured financial data with 14 sections for a specific stockid via async job execution,
**so that** I can select and reorder sections for news generation in NEW or OLD_NEW data modes.

## Acceptance Criteria

1. `StructuredDataService` created in `backend/app/services/structured_data_service.py`
2. Subprocess execution: `python generate_full_report.py {stockid} {exchange}` via asyncio.create_subprocess_exec
3. Backend endpoint `POST /api/data/structured/generate` with required stockid, optional sections/section_order/exchange
4. Parse 14 sections from script output separated by 80 "=" characters
5. **Section titles extracted from output** (first line after separator):
   - Company Info, Income Statement (Q), Income Statement (A), Balance Sheet, Cash Flow, Ratios, Valuation, Shareholding, Stock Performance, Technical Analysis, Quality Assessment, Financial Trend, Proprietary Score, Peer Comparison
6. Filter sections based on `sections` parameter (List[int] of section numbers 1-14)
7. Reorder sections based on `section_order` parameter (List[int])
8. Timeout: 60 seconds via asyncio.wait_for (script typically takes 8-15 seconds)
9. Script errors caught: non-zero exit codes, stderr output, timeout errors with actionable messages
10. **No caching** - always generate fresh data (time-sensitive during market hours 9:15 AM - 3:30 PM IST)
11. Execution logged: stockid, execution time, section count, errors
12. **Async job pattern**: Return job_id (UUID), store job status in MongoDB, polling endpoint GET /api/data/structured/jobs/{job_id}
13. Job statuses: pending → running → completed/failed
14. Unit tests: script execution, section parsing, filtering, reordering, error handling
15. Integration tests: invalid stockid, script failures, timeout scenarios, job status polling

## Tasks / Subtasks

- [ ] Task 1: Create StructuredDataService (AC: 1, 2, 4, 8)
  - [ ] Create backend/app/services/structured_data_service.py
  - [ ] Implement generate_report() method with subprocess execution
  - [ ] Implement _parse_sections() to split by 80 "=" separators
  - [ ] Add 60-second timeout with asyncio.wait_for
  - [ ] Configure script path to structured_report_builder/generate_full_report.py
- [ ] Task 2: Implement section filtering and reordering (AC: 6, 7)
  - [ ] Filter sections based on sections parameter (List[int])
  - [ ] Reorder sections based on section_order parameter
  - [ ] Handle default case (all 14 sections in order 1-14)
  - [ ] Validate section numbers (1-14 range)
- [ ] Task 3: Create async job pattern (AC: 12, 13)
  - [ ] Create jobs collection schema in MongoDB
  - [ ] Generate job_id (UUID) on request
  - [ ] Store job status: pending/running/completed/failed
  - [ ] Update job status throughout execution
  - [ ] Create GET /api/data/structured/jobs/{job_id} endpoint
- [ ] Task 4: Create generate endpoint (AC: 3, 10, 11)
  - [ ] Create POST /api/data/structured/generate endpoint in routers/data.py
  - [ ] Accept stockid (required), sections, section_order, exchange parameters
  - [ ] Return job_id immediately for async polling
  - [ ] Execute StructuredDataService.generate_report() in background
  - [ ] Log execution time, section count, stockid
  - [ ] Set Cache-Control: no-cache header
- [ ] Task 5: Add error handling (AC: 9, 11)
  - [ ] Catch subprocess non-zero exit codes
  - [ ] Catch asyncio.TimeoutError with clear message
  - [ ] Catch parsing errors (malformed output)
  - [ ] Store errors in job record
  - [ ] Return actionable error messages via job status
- [ ] Task 6: Write unit and integration tests (AC: 14, 15)
  - [ ] Test subprocess execution with sample stockid
  - [ ] Test section parsing with 80 "=" separators
  - [ ] Test section filtering (subset of 14)
  - [ ] Test section reordering
  - [ ] Test timeout mechanism (mock slow script)
  - [ ] Test error scenarios: invalid stockid, script failure
  - [ ] Test job status polling flow

## Dev Notes

### Prerequisites from Previous Stories

[Source: Story 2.3 v2.0 - Data Retrieval and Data Mode Selection]

Before starting this story, ensure Story 2.3 v2.0 is complete:
- `GET /api/triggers/{trigger_name}/data?stockid={stockid}` endpoint exists for OLD data retrieval
- `GET /api/triggers/{trigger_name}/prompts` endpoint exists for pre-population
- Data mode selection (OLD/NEW/OLD_NEW) implemented in frontend
- stockid input field functional and validated
- Frontend WorkflowContext stores data_mode and stockid

**Note**: Story 2.4 focuses on NEW data generation only. OLD data retrieval is handled by Story 2.3. Data merging (OLD_NEW mode) will be handled by Story 2.6.

### Project Structure & File Locations

[Source: docs/architecture/news-cms-workflow.md - System Components]

**Backend files to create** in `backend/app/`:
```
backend/app/
├── routers/
│   └── data.py                         # Update: Add /api/data/structured/* endpoints (MODIFY)
├── services/
│   └── structured_data_service.py      # StructuredDataService for generate_full_report.py (NEW)
├── models/
│   └── structured_data.py              # Pydantic models for GenerateRequest, JobResponse (NEW)
└── schemas/
    └── job.py                          # Job status schema for MongoDB (NEW)
```

**Python script location**:
```
structured_report_builder/
└── generate_full_report.py             # Python script that generates 14-section reports
```

**MongoDB collections**:
- `structured_data_jobs` - Job tracking for async execution (pending/running/completed/failed)

### Technology Stack Details

[Source: docs/architecture/news-cms-workflow.md - Core Architecture Principles]

**Backend Stack for Structured Data Generation**:
- **Python**: 3.11+ for subprocess execution
- **asyncio**: Async subprocess with asyncio.create_subprocess_exec and asyncio.wait_for
- **FastAPI**: Async endpoints for generate and job status
- **Motor**: Async MongoDB driver for job tracking (structured_data_jobs collection)
- **Pydantic**: Request/response validation (GenerateRequest, JobResponse, SectionData)
- **uuid**: Job ID generation
- **logging**: Execution time, section count, errors

### StructuredDataService Specification

[Source: docs/architecture/news-cms-workflow.md - System Components - StructuredDataService]

**File: backend/app/services/structured_data_service.py**

Complete implementation for executing generate_full_report.py and parsing 14 sections:

```python
from typing import Dict, Any, List, Optional
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from motor.motor_asyncio import AsyncIOMotorDatabase

logger = logging.getLogger(__name__)

class ScriptExecutionError(Exception):
    """Raised when generate_full_report.py fails"""
    pass

class StructuredDataService:
    """
    Service for generating structured financial reports from generate_full_report.py
    Executes Python script via subprocess and parses 14 sections
    """

    def __init__(self, db: AsyncIOMotorDatabase):
        """
        Initialize service

        Args:
            db: MongoDB database instance for job tracking
        """
        self.db = db
        # Path to generate_full_report.py script
        self.script_path = Path(__file__).parent.parent.parent / "structured_report_builder" / "generate_full_report.py"
        self.timeout = 60.0  # 60 second timeout (script takes 8-15s typically)

    async def generate_report(
        self,
        stockid: int,
        sections: Optional[List[int]] = None,
        section_order: Optional[List[int]] = None,
        exchange: int = 0
    ) -> Dict[str, Any]:
        """
        Generate structured report for stockid via subprocess execution

        Args:
            stockid: Stock ID to generate report for (required, positive integer)
            sections: List of section numbers to include (1-14), defaults to all
            section_order: Order to arrange sections, defaults to 1-14
            exchange: Exchange code (default 0 for NSE)

        Returns:
            {
                "stockid": int,
                "sections": {
                    "1": {"title": str, "content": str},
                    "2": {"title": str, "content": str},
                    ...
                },
                "section_order": List[int],
                "metadata": {
                    "generated_at": datetime,
                    "script_execution_time": float,
                    "data_source": "generate_full_report.py"
                }
            }

        Raises:
            ScriptExecutionError: If script fails with non-zero exit code
            TimeoutError: If execution exceeds 60 seconds
            ValueError: If parsing fails or output is malformed
        """
        start_time = datetime.utcnow()

        try:
            logger.info(f"Generating structured report for stockid={stockid}, exchange={exchange}")

            # Execute Python script via subprocess
            process = await asyncio.create_subprocess_exec(
                "python",
                str(self.script_path),
                str(stockid),
                str(exchange),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            # Wait for output with 60-second timeout
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=self.timeout
                )
            except asyncio.TimeoutError:
                # Kill process on timeout
                process.kill()
                await process.wait()
                elapsed = (datetime.utcnow() - start_time).total_seconds()
                error_msg = f"Script execution timeout after {elapsed:.2f}s (limit: {self.timeout}s)"
                logger.error(error_msg)
                raise TimeoutError(error_msg)

            # Check exit code
            if process.returncode != 0:
                error_msg = stderr.decode() if stderr else "Unknown error"
                logger.error(f"Script failed with exit code {process.returncode}: {error_msg}")
                raise ScriptExecutionError(f"generate_full_report.py failed: {error_msg}")

            # Parse output (14 sections separated by 80 "=" characters)
            raw_output = stdout.decode()
            parsed_sections = self._parse_sections(raw_output)

            # Filter sections if specified
            if sections:
                parsed_sections = {
                    k: v for k, v in parsed_sections.items()
                    if int(k) in sections
                }

            # Reorder sections if specified
            if section_order:
                # Reorder dictionary based on section_order
                reordered = {}
                for section_num in section_order:
                    key = str(section_num)
                    if key in parsed_sections:
                        reordered[key] = parsed_sections[key]
                parsed_sections = reordered

            elapsed = (datetime.utcnow() - start_time).total_seconds()
            logger.info(
                f"Report generated successfully: stockid={stockid}, "
                f"sections={len(parsed_sections)}, time={elapsed:.2f}s"
            )

            return {
                "stockid": stockid,
                "sections": parsed_sections,
                "section_order": section_order or list(range(1, 15)),
                "metadata": {
                    "generated_at": datetime.utcnow(),
                    "script_execution_time": elapsed,
                    "data_source": "generate_full_report.py"
                }
            }

        except (ScriptExecutionError, TimeoutError):
            raise
        except Exception as e:
            elapsed = (datetime.utcnow() - start_time).total_seconds()
            logger.error(f"Unexpected error after {elapsed:.2f}s: {str(e)}")
            raise ValueError(f"Failed to generate report: {str(e)}")

    def _parse_sections(self, raw_output: str) -> Dict[str, Dict[str, str]]:
        """
        Parse 14 sections from script output

        Args:
            raw_output: Raw stdout from generate_full_report.py

        Returns:
            Dictionary of sections: {"1": {"title": ..., "content": ...}, ...}

        Raises:
            ValueError: If parsing fails or output is malformed
        """
        try:
            # Split by 80 "=" characters
            sections = {}
            section_texts = raw_output.split("=" * 80)

            # Script output format:
            # ================================================================================
            # Section Title
            # Section content...
            # ================================================================================
            # Next Section Title
            # ...

            for idx, section_text in enumerate(section_texts, start=1):
                if idx > 14:
                    break  # Only process first 14 sections

                if not section_text.strip():
                    continue  # Skip empty sections

                # Extract title and content
                lines = section_text.strip().split("\n")
                title = lines[0].strip() if lines else f"Section {idx}"
                content = "\n".join(lines[1:]).strip() if len(lines) > 1 else ""

                sections[str(idx)] = {
                    "title": title,
                    "content": content
                }

            # Validate we got 14 sections
            if len(sections) != 14:
                logger.warning(
                    f"Expected 14 sections, got {len(sections)}. "
                    f"Output may be incomplete."
                )

            return sections

        except Exception as e:
            logger.error(f"Failed to parse sections: {str(e)}")
            raise ValueError(f"Failed to parse script output: {str(e)}")
```

### Backend API Specification (Updated for News CMS Workflow)

[Source: docs/architecture/news-cms-workflow.md - API Endpoints Summary]

This story implements TWO endpoints for async job-based structured data generation:

#### 1. POST /api/data/structured/generate

Creates async job to generate structured report from generate_full_report.py.

**File: backend/app/routers/data.py** (Update)

Add structured data endpoints to existing data router:

```python
# Add to existing routers/data.py

from ..services.structured_data_service import StructuredDataService, ScriptExecutionError
from fastapi import BackgroundTasks, Response
import uuid
from datetime import datetime

# Pydantic models
class GenerateRequest(BaseModel):
    stockid: int = Field(..., gt=0, description="Stock ID (required)")
    sections: Optional[List[int]] = Field(None, description="Section numbers 1-14 to include")
    section_order: Optional[List[int]] = Field(None, description="Order for sections")
    exchange: int = Field(0, description="Exchange code (default 0 for NSE)")

class JobResponse(BaseModel):
    job_id: str
    status: str  # "pending" | "running" | "completed" | "failed"
    created_at: datetime
    message: str

class JobStatusResponse(BaseModel):
    job_id: str
    status: str
    stockid: Optional[int]
    result: Optional[Dict[str, Any]]  # Contains sections data when completed
    error: Optional[str]
    created_at: datetime
    updated_at: datetime
    execution_time: Optional[float]

# Background task to execute generation
async def execute_generation_task(job_id: str, stockid: int, sections: Optional[List[int]],
                                    section_order: Optional[List[int]], exchange: int, db):
    """Background task to execute generate_full_report.py"""
    try:
        # Update job status to running
        await db.structured_data_jobs.update_one(
            {"job_id": job_id},
            {"$set": {"status": "running", "updated_at": datetime.utcnow()}}
        )

        # Execute StructuredDataService
        service = StructuredDataService(db)
        result = await service.generate_report(
            stockid=stockid,
            sections=sections,
            section_order=section_order,
            exchange=exchange
        )

        # Update job status to completed
        await db.structured_data_jobs.update_one(
            {"job_id": job_id},
            {
                "$set": {
                    "status": "completed",
                    "result": result,
                    "execution_time": result["metadata"]["script_execution_time"],
                    "updated_at": datetime.utcnow()
                }
            }
        )

    except (ScriptExecutionError, TimeoutError, ValueError) as e:
        # Update job status to failed
        await db.structured_data_jobs.update_one(
            {"job_id": job_id},
            {
                "$set": {
                    "status": "failed",
                    "error": str(e),
                    "updated_at": datetime.utcnow()
                }
            }
        )

@router.post("/data/structured/generate", response_model=JobResponse)
async def generate_structured_data(
    request: GenerateRequest,
    background_tasks: BackgroundTasks,
    response: Response,
    db = Depends(get_database)
):
    """
    Generate structured report for stockid via async job pattern

    Returns job_id immediately for polling via GET /data/structured/jobs/{job_id}

    Args:
        request: GenerateRequest with stockid, optional sections/section_order/exchange

    Returns:
        JobResponse with job_id and status "pending"
    """
    try:
        # Generate unique job ID
        job_id = str(uuid.uuid4())

        # Create job record in MongoDB
        job_doc = {
            "job_id": job_id,
            "status": "pending",
            "stockid": request.stockid,
            "sections": request.sections,
            "section_order": request.section_order,
            "exchange": request.exchange,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
            "result": None,
            "error": None,
            "execution_time": None
        }

        await db.structured_data_jobs.insert_one(job_doc)

        # Add background task to execute generation
        background_tasks.add_task(
            execute_generation_task,
            job_id=job_id,
            stockid=request.stockid,
            sections=request.sections,
            section_order=request.section_order,
            exchange=request.exchange,
            db=db
        )

        # Set no-cache header
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"

        logger.info(f"Created job {job_id} for stockid={request.stockid}")

        return JobResponse(
            job_id=job_id,
            status="pending",
            created_at=job_doc["created_at"],
            message=f"Job created for stockid {request.stockid}. Poll /data/structured/jobs/{job_id} for status."
        )

    except Exception as e:
        logger.error(f"Failed to create generation job: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create job: {str(e)}")
```

#### 2. GET /api/data/structured/jobs/{job_id}

Polls job status for async generation.

**File: backend/app/routers/data.py** (Update - add to same file)

```python
@router.get("/data/structured/jobs/{job_id}", response_model=JobStatusResponse)
async def get_job_status(
    job_id: str,
    db = Depends(get_database)
):
    """
    Get status of structured data generation job

    Args:
        job_id: UUID of the job

    Returns:
        JobStatusResponse with current status, result (if completed), or error (if failed)
    """
    try:
        # Find job in database
        job = await db.structured_data_jobs.find_one({"job_id": job_id})

        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        return JobStatusResponse(
            job_id=job["job_id"],
            status=job["status"],
            stockid=job.get("stockid"),
            result=job.get("result"),
            error=job.get("error"),
            created_at=job["created_at"],
            updated_at=job["updated_at"],
            execution_time=job.get("execution_time")
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to fetch job status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch job: {str(e)}")
```

### Data Models

[Source: docs/architecture/news-cms-workflow.md - System Components]

**File: backend/app/models/structured_data.py** (New)

```python
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, List
from datetime import datetime

class SectionData(BaseModel):
    """Individual section from generate_full_report.py"""
    title: str
    content: str

class GenerateRequest(BaseModel):
    """Request model for POST /data/structured/generate"""
    stockid: int = Field(..., gt=0, description="Stock ID (required)")
    sections: Optional[List[int]] = Field(None, description="Section numbers 1-14")
    section_order: Optional[List[int]] = Field(None, description="Order for sections")
    exchange: int = Field(0, description="Exchange code (0=NSE)")

class JobResponse(BaseModel):
    """Response model for job creation"""
    job_id: str
    status: str
    created_at: datetime
    message: str

class JobStatusResponse(BaseModel):
    """Response model for job status polling"""
    job_id: str
    status: str  # pending/running/completed/failed
    stockid: Optional[int]
    result: Optional[Dict[str, Any]]
    error: Optional[str]
    created_at: datetime
    updated_at: datetime
    execution_time: Optional[float]
```

**File: backend/app/schemas/job.py** (New)

MongoDB schema for job tracking:

```python
from datetime import datetime
from typing import Optional, List, Dict, Any

# MongoDB document structure for structured_data_jobs collection
job_schema = {
    "job_id": str,  # UUID
    "status": str,  # "pending" | "running" | "completed" | "failed"
    "stockid": int,
    "sections": Optional[List[int]],  # None = all 14
    "section_order": Optional[List[int]],  # None = 1-14
    "exchange": int,  # Default 0
    "result": Optional[Dict[str, Any]],  # Populated on completion
    "error": Optional[str],  # Populated on failure
    "created_at": datetime,
    "updated_at": datetime,
    "execution_time": Optional[float]  # Seconds
}

# Index for efficient queries
# db.structured_data_jobs.create_index("job_id", unique=True)
# db.structured_data_jobs.create_index("created_at", expireAfterSeconds=86400)  # 24hr TTL
```

### Environment Variables

[Source: docs/architecture/news-cms-workflow.md - Core Architecture Principles]

Update `.env.example`:
```
# Structured Data Generation (Story 2.4)
STRUCTURED_DATA_TIMEOUT=60  # Timeout in seconds (default: 60)
GENERATE_FULL_REPORT_PATH=../structured_report_builder/generate_full_report.py

# MongoDB Job TTL (optional)
JOB_RETENTION_HOURS=24  # Auto-delete completed jobs after 24 hours
```

### Implementation Notes

**Important Design Decisions**:

1. **Async Job Pattern**: Long-running operation (8-15s) handled via background task with job_id polling to avoid HTTP timeout.

2. **60-Second Timeout**: Script typically takes 8-15 seconds; 60s timeout provides safe buffer.

3. **No Caching**: Always generate fresh data due to market hours sensitivity (9:15 AM - 3:30 PM IST).

4. **Section Filtering**: Filter sections AFTER parsing to avoid script modifications.

5. **Job Retention**: MongoDB TTL index auto-deletes jobs after 24 hours to prevent bloat.

6. **14 Section Parsing**: Split by 80 "=" characters, extract title from first line, content from remaining lines.

7. **Error Handling**: Script failures (non-zero exit code, timeout, parsing errors) stored in job record for frontend display.

### Testing

[Source: PRD - Technical Assumptions - Testing Requirements]

**Testing Strategy for Story 2.4** (Updated for News CMS Workflow):

Focus on StructuredDataService, job pattern, and generate_full_report.py integration.

**Unit Tests** (backend/tests/test_structured_data_service.py):

```python
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
from app.parsers.adapter import ParserAdapter, DefaultParser
import asyncio
import json

@pytest.mark.asyncio
async def test_default_parser_success():
    """Test default parser with valid raw data"""
    parser = DefaultParser(timeout=10)

    raw_data = {
        "1": {"earnings": {"eps": 1.50}},
        "2": {"revenue": {"total": 100000000}}
    }

    result = await parser.parse(raw_data)

    assert "sections" in result
    assert "metadata" in result
    assert "1" in result["sections"]
    assert "2" in result["sections"]
    assert result["sections"]["1"]["section_name"] == "Earnings Summary"
    assert result["sections"]["2"]["section_name"] == "Revenue Breakdown"

@pytest.mark.asyncio
async def test_parser_timeout():
    """Test parser timeout mechanism"""
    parser = DefaultParser(timeout=1)  # 1 second timeout

    # Mock slow parser execution
    async def slow_parse(*args, **kwargs):
        await asyncio.sleep(2)  # Exceeds timeout
        return {}

    with patch.object(parser, 'parse', side_effect=slow_parse):
        with pytest.raises(asyncio.TimeoutError):
            await parser.parse({"1": {}})

@pytest.mark.asyncio
async def test_parser_with_empty_data():
    """Test parser handles empty raw data"""
    parser = DefaultParser()

    result = await parser.parse({})

    assert "sections" in result
    assert len(result["sections"]) == 0

@pytest.mark.asyncio
async def test_parser_skips_null_sections():
    """Test parser skips sections with null data"""
    parser = DefaultParser()

    raw_data = {
        "1": {"earnings": {"eps": 1.50}},
        "2": None,  # Null data should be skipped
        "3": {"price": 150.00}
    }

    result = await parser.parse(raw_data)

    assert "sections" in result
    assert "1" in result["sections"]
    assert "2" not in result["sections"]  # Should be skipped
    assert "3" in result["sections"]

@pytest.mark.asyncio
async def test_module_execution_mode():
    """Test parser module import execution mode"""
    # Create mock parser module
    mock_parser_path = "/tmp/test_parser.py"

    parser = ParserAdapter(mock_parser_path, timeout=10)

    # Mock module loading
    with patch('importlib.util.spec_from_file_location') as mock_spec:
        mock_module = MagicMock()
        mock_module.parse = lambda data: {"sections": {}}
        mock_spec.return_value = MagicMock()
        mock_spec.return_value.loader = MagicMock()

        # Test would require actual file for full execution
        # This tests the structure
        assert parser.execution_mode in ["module", "subprocess"]

@pytest.mark.asyncio
async def test_subprocess_execution_mode():
    """Test parser subprocess execution mode"""
    # Would require actual executable script
    # Mock test to verify structure
    pass

@pytest.mark.asyncio
async def test_parser_error_handling():
    """Test parser handles exceptions gracefully"""
    parser = DefaultParser()

    # Mock parser that raises exception
    async def failing_parse(*args, **kwargs):
        raise ValueError("Parser execution failed")

    with patch.object(parser, 'parse', side_effect=failing_parse):
        with pytest.raises(ValueError):
            await parser.parse({"1": {}})
```

**Unit Tests** (backend/tests/test_parser_service.py):

```python
import pytest
from unittest.mock import AsyncMock, patch
from app.services.parser_service import ParserService
from app.parsers.adapter import DefaultParser

@pytest.mark.asyncio
async def test_parse_data_success(db):
    """Test successful data parsing"""
    service = ParserService(db)

    raw_data = {
        "1": {"earnings": {"eps": 1.50}},
        "2": {"revenue": {"total": 100000000}}
    }

    result = await service.parse_data(
        trigger_id="trigger_001",
        raw_data=raw_data
    )

    assert "structured_data" in result
    assert "errors" in result
    assert len(result["errors"]) == 0
    assert "sections" in result["structured_data"]

@pytest.mark.asyncio
async def test_parse_data_with_timeout_error(db):
    """Test parser service handles timeout errors"""
    service = ParserService(db)

    with patch.object(DefaultParser, 'parse', side_effect=TimeoutError("Timeout")):
        result = await service.parse_data(
            trigger_id="trigger_001",
            raw_data={"1": {}}
        )

        assert len(result["errors"]) > 0
        assert result["errors"][0]["error_type"] == "timeout"

@pytest.mark.asyncio
async def test_parse_data_with_validation_error(db):
    """Test parser service handles validation errors"""
    service = ParserService(db)

    with patch.object(DefaultParser, 'parse', side_effect=ValueError("Invalid output")):
        result = await service.parse_data(
            trigger_id="trigger_001",
            raw_data={"1": {}}
        )

        assert len(result["errors"]) > 0
        assert result["errors"][0]["error_type"] == "validation"

@pytest.mark.asyncio
async def test_parse_data_with_empty_input(db):
    """Test parser service handles empty raw data"""
    service = ParserService(db)

    with pytest.raises(ValueError):
        await service.parse_data(
            trigger_id="trigger_001",
            raw_data={}
        )

@pytest.mark.asyncio
async def test_parse_endpoint(client):
    """Test POST /api/triggers/:id/data/parse endpoint"""
    response = await client.post(
        "/api/triggers/trigger_001/data/parse",
        json={
            "raw_data": {
                "1": {"earnings": {"eps": 1.50}},
                "2": {"revenue": {"total": 100000000}}
            }
        }
    )

    assert response.status_code == 200
    data = response.json()
    assert "structured_data" in data
    assert "errors" in data

@pytest.mark.asyncio
async def test_parse_endpoint_invalid_trigger(client):
    """Test parse endpoint with non-existent trigger"""
    response = await client.post(
        "/api/triggers/invalid_trigger/data/parse",
        json={"raw_data": {"1": {}}}
    )

    assert response.status_code == 404
```

**Integration Tests** (backend/tests/integration/test_parser_integration.py):

```python
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.integration
@pytest.mark.asyncio
async def test_full_parse_workflow():
    """Test complete data fetch and parse workflow"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # 1. Fetch raw data
        fetch_response = await client.post(
            "/api/triggers/trigger_001/data/fetch",
            json={
                "stock_id": "AAPL",
                "section_ids": ["1", "2", "3"]
            }
        )
        assert fetch_response.status_code == 200
        fetch_data = fetch_response.json()

        # 2. Parse raw data
        parse_response = await client.post(
            "/api/triggers/trigger_001/data/parse",
            json={"raw_data": fetch_data["raw_data"]}
        )
        assert parse_response.status_code == 200
        parse_data = parse_response.json()

        # 3. Verify structured data
        assert "structured_data" in parse_data
        assert "errors" in parse_data
        assert "sections" in parse_data["structured_data"]

@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_with_malformed_json():
    """Test parser handles malformed JSON gracefully"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.post(
            "/api/triggers/trigger_001/data/parse",
            json={"raw_data": {"1": "invalid_json_structure"}}
        )

        # Should still return 200 with errors
        assert response.status_code == 200
        data = response.json()
        # Parser should handle gracefully
        assert "structured_data" in data

@pytest.mark.integration
@pytest.mark.asyncio
async def test_parse_with_missing_fields():
    """Test parser handles missing required fields"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.post(
            "/api/triggers/trigger_001/data/parse",
            json={"raw_data": {"1": {}}}  # Empty data
        )

        assert response.status_code == 200
        # Parser should handle empty data
        data = response.json()
        assert "structured_data" in data
```

**Manual Verification Checklist** (Updated for News CMS Workflow):
1. POST /api/data/structured/generate endpoint accessible and returns job_id
2. GET /api/data/structured/jobs/{job_id} endpoint polls job status
3. Job statuses transition correctly: pending → running → completed/failed
4. generate_full_report.py executes successfully for valid stockid (e.g., 513374)
5. 14 sections parsed correctly from script output (80 "=" separator)
6. Section titles extracted from first line after separator
7. Section filtering works (only requested sections returned)
8. Section reordering works (order matches section_order parameter)
9. 60-second timeout mechanism activates for slow scripts
10. Script failures (non-zero exit code) caught and stored in job error field
11. Job records created in structured_data_jobs collection
12. No-cache header set on generate endpoint
13. Execution time logged and stored in job record
14. API docs show both endpoints at /docs
15. Invalid stockid returns failed job status with error message
16. Job retention: old jobs auto-deleted after 24 hours (if TTL index configured)

## News CMS Workflow Updates

This story has been updated to integrate with the News CMS Workflow Feature (Epic 6). Key changes:

### Major Changes:
- **NEW Endpoint**: `POST /api/data/structured/generate` replaces old `POST /api/triggers/:id/data/parse`
- **Async Job Pattern**: Added job_id polling via `GET /api/data/structured/jobs/{job_id}`
- **Specific Script Integration**: Replaced generic ParserAdapter with StructuredDataService for generate_full_report.py
- **14 Sections**: Parse by 80 "=" separator, extract title/content
- **60s Timeout**: Increased from 10s to accommodate script execution time
- **Section Filtering & Reordering**: Support section selection and drag-drop reordering from Story 2.5
- **MongoDB Job Tracking**: structured_data_jobs collection for async execution
- **No Caching**: Always generate fresh data for market hours sensitivity

### Removed:
- Generic ParserAdapter with module/subprocess modes (~270 lines)
- DefaultParser mock implementation
- parser_service.py and parsers/adapter.py files
- Old section name mappings (Earnings Summary, Revenue Breakdown, etc.)

### Added:
- StructuredDataService with subprocess execution (~200 lines)
- Job pattern implementation with background tasks
- MongoDB job schema and TTL indexing
- Section filtering/reordering logic
- ScriptExecutionError exception class

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-29 | 1.0 | Initial story created from Epic 2 | Sarah (PO) |
| 2025-10-29 | 1.1 | Enriched with full architectural context and testing standards | Bob (SM) |
| 2025-10-30 | 2.0 | **MAJOR UPDATE**: Integrated News CMS Workflow Feature - replaced generic parser with generate_full_report.py, added async job pattern, 60s timeout, section filtering/reordering | Bob (SM) |

## Dev Agent Record

*To be populated during implementation*

### Agent Model Used

*To be filled by dev agent*

### Debug Log References

*To be filled by dev agent*

### Completion Notes List

*To be filled by dev agent*

### File List

*To be filled by dev agent*

## QA Results

*To be filled by QA agent*
