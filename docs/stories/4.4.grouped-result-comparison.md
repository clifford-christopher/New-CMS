# Story 4.4: Grouped Result Comparison (By Prompt Type ‚Üí Model)

## Status

Complete

## Story

**As a** content manager,
**I want** to view generated news grouped by prompt type then by model,
**so that** I can easily compare quality, tone, and accuracy across different audiences and select the best model for each type.

## Acceptance Criteria

1. "Generation Results" section displays outputs grouped hierarchically: Prompt Type ‚Üí Models (AC: 1, 9)
2. Each prompt type section has colored header: üí∞ Paid (blue), üÜì Unpaid (green), üï∑Ô∏è Crawler (orange) (AC: 2)
3. Within each prompt type, models displayed side-by-side in columns (2-3 columns on desktop) (AC: 3)
4. Each model column shows: model name, generated news text, metadata (tokens, actual cost, latency) (AC: 4)
5. Metadata displayed below each result: üéØ Tokens: 456 | ‚è±Ô∏è Time: 8.3s | üí∞ Cost: $0.08 (AC: 5)
6. Columns aligned vertically within each prompt type group for easy visual comparison (AC: 6)
7. Responsive design: 2-3 columns on desktop (1200px+), single column (stacked) on tablet (768-1199px) (AC: 7, 13)
8. Meets FR21 and FR22: grouped display by type with detailed metadata per generation (AC: 8)
9. Syntax highlighting or formatting for generated text (markdown rendering if applicable) (AC: 9)
10. "Copy" button per result allows copying generated news to clipboard (AC: 10)
11. Visual indicators for outliers within each prompt type group: longest/shortest output, highest/lowest cost, fastest/slowest generation (AC: 11)
12. Collapsible sections per prompt type to reduce scrolling when viewing multiple types (AC: 12)
13. Meets NFR6: responsive design works on desktop and tablet (AC: 13)
14. Results remain visible while editing prompt for iterative refinement (AC: 14)
15. Meets FR40: actual token usage, generation time, and cost displayed prominently per result (AC: 15)

## Tasks / Subtasks

- [ ] Task 1: Create ResultsDisplay component with hierarchical grouping (AC: 1, 2, 3, 12)
  - [ ] Create frontend/src/components/config/ResultsDisplay.tsx
  - [ ] Implement hierarchical data structure: {prompt_type: {model_id: result}}
  - [ ] Render colored headers per prompt type (üí∞, üÜì, üï∑Ô∏è)
  - [ ] Implement collapsible/expandable sections per type
  - [ ] Grid layout for side-by-side model comparison
  - [ ] Pass results from GenerationContext
- [ ] Task 2: Create ResultCard component for individual model results (AC: 4, 5, 9, 10, 15)
  - [ ] Create frontend/src/components/config/ResultCard.tsx
  - [ ] Display model name and generated text
  - [ ] Render metadata section with icons (üéØ Tokens, ‚è±Ô∏è Time, üí∞ Cost)
  - [ ] Implement "Copy to Clipboard" button
  - [ ] Markdown rendering for generated text if applicable
  - [ ] Status indicators (success/failed)
  - [ ] Error message display for failed generations
- [ ] Task 3: Implement responsive grid layout (AC: 3, 6, 7, 13)
  - [ ] 2-3 columns on desktop (1200px+)
  - [ ] 1-2 columns on tablet (768-1199px)
  - [ ] Single column on mobile
  - [ ] Vertical alignment within prompt type groups
  - [ ] Use React-Bootstrap Grid system
- [ ] Task 4: Add outlier visual indicators (AC: 11)
  - [ ] Calculate min/max for tokens, cost, latency per prompt type
  - [ ] Highlight longest/shortest output
  - [ ] Highlight highest/lowest cost
  - [ ] Highlight fastest/slowest generation
  - [ ] Visual badges or border highlights
- [ ] Task 5: Integrate with GenerationContext results (AC: 1, 14)
  - [ ] Subscribe to GenerationContext results state
  - [ ] Display results after generation completes
  - [ ] Maintain visibility during prompt editing
  - [ ] Handle empty state (no results yet)
- [ ] Task 6: Write unit and integration tests (AC: all)
  - [ ] Test hierarchical grouping and sorting
  - [ ] Test responsive grid layout
  - [ ] Test copy-to-clipboard functionality
  - [ ] Test outlier calculation and highlighting
  - [ ] Test metadata display formatting
  - [ ] Test collapsible section toggle

## Dev Notes

### Prerequisites from Previous Stories

[Source: Story 4.3 - Completion]

Before starting this story, ensure Story 4.3 is complete:
- GenerationContext with results per type per model
- GenerationPanel with generation status
- Backend generation endpoint returning results grouped by type
- MongoDB storage of generation results

**Dependencies**:
- GenerationContext provides results: {prompt_type: {model_id: {generated_text, token_count, cost, latency}}}
- Results available after generation completes (isGenerating = false)

### Project Structure & File Locations

[Source: architecture.md - Source Tree and Module Organization]

**Frontend files to create** in `frontend/src/`:
```
frontend/src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ ResultsDisplay.tsx           # Main results container with grouping (NEW)
‚îÇ       ‚îî‚îÄ‚îÄ ResultCard.tsx               # Individual model result card (NEW)
‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îî‚îÄ‚îÄ GenerationContext.tsx            # (From Story 4.3)
‚îî‚îÄ‚îÄ lib/
    ‚îî‚îÄ‚îÄ result-utils.ts                  # Utility functions for formatting (NEW)
```

### Technology Stack Details

[Source: architecture.md - High Level Architecture - Planned Tech Stack]

**Frontend Stack**:
- **React-Bootstrap**: Grid system, Card components, Badge, Button
- **React-Markdown**: Markdown rendering for generated text
- **Clipboard API**: Browser native copy-to-clipboard functionality
- **TypeScript**: Type safety for result structures

### Frontend Results Display Component

[Source: architecture.md - Source Tree and Module Organization - Frontend Module Structure]

**File: frontend/src/components/config/ResultsDisplay.tsx** (NEW)

Hierarchical results grouping with collapsible sections:

```typescript
'use client';

import { useState } from 'react';
import { Card, Row, Col, Button, Badge, Alert } from 'react-bootstrap';
import { ChevronDown, ChevronRight } from 'react-bootstrap-icons';
import ReactMarkdown from 'react-markdown';
import { useGeneration } from '@/contexts/GenerationContext';
import ResultCard from './ResultCard';

interface PromptTypeGroup {
  type: string;
  models: Record<string, any>;
  isExpanded: boolean;
}

const PROMPT_TYPE_CONFIG = {
  paid: {
    icon: 'üí∞',
    label: 'Paid',
    color: 'primary',
    bgColor: '#e7f1ff'
  },
  unpaid: {
    icon: 'üÜì',
    label: 'Free Tier',
    color: 'success',
    bgColor: '#e7f5ea'
  },
  crawler: {
    icon: 'üï∑Ô∏è',
    label: 'Crawler',
    color: 'warning',
    bgColor: '#fff3e0'
  }
} as const;

export default function ResultsDisplay() {
  const { results, isGenerating, failedGenerations } = useGeneration();
  const [expandedTypes, setExpandedTypes] = useState<Set<string>>(
    new Set(Object.keys(results))
  );

  if (Object.keys(results).length === 0) {
    return null;
  }

  const toggleType = (promptType: string) => {
    const newExpanded = new Set(expandedTypes);
    if (newExpanded.has(promptType)) {
      newExpanded.delete(promptType);
    } else {
      newExpanded.add(promptType);
    }
    setExpandedTypes(newExpanded);
  };

  // Calculate outliers per prompt type
  const calculateOutliers = (models: Record<string, any>) => {
    const stats = {
      maxTokens: 0,
      minTokens: Infinity,
      maxCost: 0,
      minCost: Infinity,
      maxLatency: 0,
      minLatency: Infinity,
      maxLength: 0,
      minLength: Infinity
    };

    Object.values(models).forEach((result: any) => {
      if (result.status === 'complete') {
        stats.maxTokens = Math.max(stats.maxTokens, result.token_count || 0);
        stats.minTokens = Math.min(stats.minTokens, result.token_count || Infinity);
        stats.maxCost = Math.max(stats.maxCost, result.cost || 0);
        stats.minCost = Math.min(stats.minCost, result.cost || Infinity);
        stats.maxLatency = Math.max(stats.maxLatency, result.latency || 0);
        stats.minLatency = Math.min(stats.minLatency, result.latency || Infinity);

        const textLength = result.generated_text?.length || 0;
        stats.maxLength = Math.max(stats.maxLength, textLength);
        stats.minLength = Math.min(stats.minLength, textLength);
      }
    });

    return stats;
  };

  return (
    <Card className="mt-4">
      <Card.Header>
        <h5 className="mb-0">
          {isGenerating ? '‚è≥ Generation Results (In Progress)' : '‚úÖ Generation Results'}
        </h5>
      </Card.Header>

      <Card.Body>
        {/* Summary Alert */}
        {failedGenerations > 0 && (
          <Alert variant="warning" className="mb-4">
            <strong>Warning:</strong> {failedGenerations} generation(s) failed. See details below.
          </Alert>
        )}

        {/* Grouped Results */}
        {Object.entries(results).map(([promptType, models]) => {
          const config = PROMPT_TYPE_CONFIG[promptType as keyof typeof PROMPT_TYPE_CONFIG];
          const isExpanded = expandedTypes.has(promptType);
          const outliers = calculateOutliers(models);

          return (
            <Card key={promptType} className="mb-3">
              {/* Collapsible Header */}
              <Card.Header
                style={{ backgroundColor: config?.bgColor || '#f8f9fa', cursor: 'pointer' }}
                onClick={() => toggleType(promptType)}
              >
                <div className="d-flex justify-content-between align-items-center">
                  <div className="d-flex align-items-center gap-2">
                    {isExpanded ? <ChevronDown size={20} /> : <ChevronRight size={20} />}
                    <strong className="fs-5">
                      {config?.icon} {config?.label}
                    </strong>
                    <Badge bg={config?.color}>
                      {Object.keys(models).length} models
                    </Badge>
                  </div>
                  <span className="text-muted small">
                    {Object.values(models).filter((m: any) => m.status === 'complete').length} succeeded
                  </span>
                </div>
              </Card.Header>

              {/* Expanded Content */}
              {isExpanded && (
                <Card.Body>
                  <Row>
                    {Object.entries(models).map(([modelId, result]) => (
                      <Col key={modelId} lg={6} xl={4} className="mb-3">
                        <ResultCard
                          modelId={modelId}
                          result={result}
                          outliers={outliers}
                          promptType={promptType}
                        />
                      </Col>
                    ))}
                  </Row>
                </Card.Body>
              )}
            </Card>
          );
        })}

        {/* Total Cost Summary */}
        {Object.keys(results).length > 0 && (
          <div className="mt-4 p-3 border-top">
            <TotalCostSummary results={results} />
          </div>
        )}
      </Card.Body>
    </Card>
  );
}

function TotalCostSummary({ results }: { results: Record<string, any> }) {
  let totalCost = 0;
  let totalGenerations = 0;

  Object.values(results).forEach((models: any) => {
    Object.values(models).forEach((result: any) => {
      if (result.status === 'complete') {
        totalCost += result.cost || 0;
        totalGenerations += 1;
      }
    });
  });

  return (
    <div className="d-flex justify-content-between align-items-center">
      <span>
        <strong>üìä Total Cost Summary:</strong> ${totalCost.toFixed(4)} ({totalGenerations} successful generations)
      </span>
    </div>
  );
}
```

### Frontend Result Card Component

[Source: architecture.md - Source Tree and Module Organization - Frontend Module Structure]

**File: frontend/src/components/config/ResultCard.tsx** (NEW)

Individual result display with metadata:

```typescript
'use client';

import { useState } from 'react';
import { Card, Button, Badge, Alert, Tooltip, OverlayTrigger } from 'react-bootstrap';
import { Copy, Check } from 'react-bootstrap-icons';
import ReactMarkdown from 'react-markdown';

interface ResultCardProps {
  modelId: string;
  result: {
    status: 'complete' | 'failed' | 'pending' | 'generating';
    generated_text?: string;
    token_count?: number;
    prompt_tokens?: number;
    completion_tokens?: number;
    cost?: number;
    latency?: number;
    error?: string;
  };
  outliers: Record<string, number>;
  promptType: string;
}

export default function ResultCard({
  modelId,
  result,
  outliers,
  promptType
}: ResultCardProps) {
  const [copied, setCopied] = useState(false);

  const handleCopy = async () => {
    try {
      await navigator.clipboard.writeText(result.generated_text || '');
      setCopied(true);
      setTimeout(() => setCopied(false), 2000);
    } catch (err) {
      console.error('Failed to copy:', err);
    }
  };

  const getStatusBadge = () => {
    switch (result.status) {
      case 'complete':
        return <Badge bg="success">‚úÖ Complete</Badge>;
      case 'failed':
        return <Badge bg="danger">‚ùå Failed</Badge>;
      case 'generating':
        return <Badge bg="primary">‚è≥ Generating</Badge>;
      case 'pending':
        return <Badge bg="secondary">‚è∏Ô∏è Pending</Badge>;
      default:
        return null;
    }
  };

  const getPerformanceColor = (latency?: number) => {
    if (!latency) return 'secondary';
    if (latency < 5) return 'success'; // green - fast
    if (latency < 15) return 'warning'; // yellow - medium
    return 'danger'; // red - slow
  };

  const isOutlier = (value: number, key: string) => {
    const outlierKeys: (keyof typeof outliers)[] = [
      'maxTokens', 'minTokens', 'maxCost', 'minCost',
      'maxLatency', 'minLatency', 'maxLength', 'minLength'
    ];
    return outlierKeys.some(k => outliers[k] === value);
  };

  return (
    <Card className="h-100 border-1">
      {/* Header with Model Name and Status */}
      <Card.Header className="d-flex justify-content-between align-items-center">
        <strong className="text-truncate">{modelId}</strong>
        {getStatusBadge()}
      </Card.Header>

      <Card.Body className="d-flex flex-column">
        {/* Generated Text */}
        {result.status === 'complete' && result.generated_text ? (
          <>
            <div className="mb-3 flex-grow-1" style={{ maxHeight: '300px', overflow: 'auto' }}>
              <ReactMarkdown className="text-small">
                {result.generated_text}
              </ReactMarkdown>
            </div>

            {/* Copy Button */}
            <Button
              variant="outline-secondary"
              size="sm"
              onClick={handleCopy}
              className="mb-3"
            >
              {copied ? (
                <>
                  <Check size={16} className="me-1" />
                  Copied!
                </>
              ) : (
                <>
                  <Copy size={16} className="me-1" />
                  Copy Text
                </>
              )}
            </Button>

            {/* Metadata Section */}
            <div
              className="p-2 rounded mt-3"
              style={{ backgroundColor: '#f8f9fa', fontSize: '0.875rem' }}
            >
              <div className="mb-2">
                <strong>‚ö° ACTUAL METRICS</strong>
              </div>

              {/* Tokens */}
              <div className="d-flex justify-content-between align-items-center mb-2">
                <span>üéØ Tokens: {result.token_count}</span>
                {isOutlier(result.token_count || 0, 'tokens') && (
                  <Badge bg="info" className="ms-2">
                    {(result.token_count || 0) === outliers.maxTokens ? 'MAX' : 'MIN'}
                  </Badge>
                )}
              </div>

              {/* Token Breakdown Tooltip */}
              <OverlayTrigger
                placement="top"
                overlay={
                  <Tooltip id={`tooltip-${modelId}`}>
                    Prompt: {result.prompt_tokens}, Completion: {result.completion_tokens}
                  </Tooltip>
                }
              >
                <span className="text-muted small" style={{ cursor: 'help' }}>
                  (breakdown)
                </span>
              </OverlayTrigger>

              {/* Latency */}
              <div className="d-flex justify-content-between align-items-center mb-2">
                <span>
                  ‚è±Ô∏è Time: {result.latency?.toFixed(2)}s
                </span>
                <Badge bg={getPerformanceColor(result.latency)}>
                  {result.latency! < 5 ? 'FAST' : result.latency! < 15 ? 'MEDIUM' : 'SLOW'}
                </Badge>
              </div>

              {/* Cost */}
              <div className="d-flex justify-content-between align-items-center">
                <span>üí∞ Cost: ${result.cost?.toFixed(4)}</span>
                {isOutlier(result.cost || 0, 'cost') && (
                  <Badge bg="info" className="ms-2">
                    {(result.cost || 0) === outliers.maxCost ? 'MAX' : 'MIN'}
                  </Badge>
                )}
              </div>
            </div>
          </>
        ) : result.status === 'failed' ? (
          <Alert variant="danger" className="mb-0">
            <strong>Generation Failed</strong>
            <p className="mb-0 mt-2 small">{result.error || 'Unknown error'}</p>
          </Alert>
        ) : (
          <Alert variant="secondary" className="mb-0 text-center">
            {result.status === 'generating' ? 'Generating...' : 'Waiting to generate'}
          </Alert>
        )}
      </Card.Body>
    </Card>
  );
}
```

### Implementation Notes

[Source: PRD - Epic 4: Multi-Model Generation & Testing]

**Important Design Decisions**:

1. **Hierarchical Grouping**: Results grouped by prompt type first (paid/unpaid/crawler), then by model within each type. Matches user mental model.

2. **Collapsible Sections**: Each prompt type can be collapsed to reduce scrolling, but all sections expanded by default for initial view.

3. **Outlier Highlighting**: Automatic calculation of min/max within each prompt type for tokens, cost, and latency. Helps identify best models for each audience.

4. **Responsive Grid**: 3 columns on desktop (1200px+), 2 columns on tablet, 1 column on mobile. Maintains vertical alignment within prompt type groups.

5. **Metadata Styling**: Subtle gray background (#f8f9fa) distinguishes actual metrics from generated text. Icon prefixes (üéØ, ‚è±Ô∏è, üí∞) provide visual scanning.

6. **Copy Functionality**: Native browser Clipboard API for instant feedback. "Copied!" state persists for 2 seconds.

7. **Markdown Rendering**: react-markdown package allows formatting of generated text if it includes markdown syntax.

8. **Performance Indicators**: Color-coded badges for latency (green <5s, yellow 5-15s, red >15s) provide at-a-glance performance feedback.

### Testing

[Source: PRD - Technical Assumptions - Testing Requirements]

**Unit Tests** (frontend/__tests__/components/config/ResultCard.test.tsx):

```typescript
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import ResultCard from '@/components/config/ResultCard';

describe('ResultCard Component', () => {
  const mockResult = {
    status: 'complete' as const,
    generated_text: 'Test output content',
    token_count: 456,
    prompt_tokens: 234,
    completion_tokens: 222,
    cost: 0.08,
    latency: 8.3
  };

  const mockOutliers = {
    maxTokens: 500,
    minTokens: 100,
    maxCost: 0.10,
    minCost: 0.02,
    maxLatency: 15,
    minLatency: 2,
    maxLength: 1000,
    minLength: 100
  };

  test('renders model name and status badge', () => {
    render(
      <ResultCard
        modelId="gpt-4"
        result={mockResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    expect(screen.getByText('gpt-4')).toBeInTheDocument();
    expect(screen.getByText('‚úÖ Complete')).toBeInTheDocument();
  });

  test('displays generated text', () => {
    render(
      <ResultCard
        modelId="gpt-4"
        result={mockResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    expect(screen.getByText('Test output content')).toBeInTheDocument();
  });

  test('displays metadata with icons', () => {
    render(
      <ResultCard
        modelId="gpt-4"
        result={mockResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    expect(screen.getByText(/üéØ Tokens: 456/)).toBeInTheDocument();
    expect(screen.getByText(/‚è±Ô∏è Time: 8.30s/)).toBeInTheDocument();
    expect(screen.getByText(/üí∞ Cost: \$0.0800/)).toBeInTheDocument();
  });

  test('copies text to clipboard', async () => {
    const mockClipboard = {
      writeText: jest.fn().mockResolvedValue(undefined)
    };
    Object.assign(navigator, { clipboard: mockClipboard });

    render(
      <ResultCard
        modelId="gpt-4"
        result={mockResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    const copyButton = screen.getByText('Copy Text');
    fireEvent.click(copyButton);

    await waitFor(() => {
      expect(mockClipboard.writeText).toHaveBeenCalledWith('Test output content');
    });
  });

  test('displays failed status', () => {
    const failedResult = {
      status: 'failed' as const,
      error: 'API Error: Rate limited'
    };

    render(
      <ResultCard
        modelId="claude"
        result={failedResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    expect(screen.getByText('‚ùå Failed')).toBeInTheDocument();
    expect(screen.getByText('API Error: Rate limited')).toBeInTheDocument();
  });

  test('shows performance indicators', () => {
    const fastResult = { ...mockResult, latency: 2.5 };
    const slowResult = { ...mockResult, latency: 20 };

    const { rerender } = render(
      <ResultCard
        modelId="gpt-4"
        result={fastResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    expect(screen.getByText('FAST')).toBeInTheDocument();

    rerender(
      <ResultCard
        modelId="gpt-4"
        result={slowResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    expect(screen.getByText('SLOW')).toBeInTheDocument();
  });

  test('highlights outliers', () => {
    const outlierResult = { ...mockResult, token_count: 500 }; // maxTokens
    const { container } = render(
      <ResultCard
        modelId="gpt-4"
        result={outlierResult}
        outliers={mockOutliers}
        promptType="paid"
      />
    );

    const maxBadge = screen.getByText('MAX');
    expect(maxBadge).toBeInTheDocument();
  });
});
```

**Integration Tests** (frontend/__tests__/components/config/ResultsDisplay.test.tsx):

```typescript
import { render, screen, fireEvent } from '@testing-library/react';
import ResultsDisplay from '@/components/config/ResultsDisplay';
import { GenerationProvider } from '@/contexts/GenerationContext';

describe('ResultsDisplay Component', () => {
  const mockResults = {
    paid: {
      'gpt-4': {
        status: 'complete',
        generated_text: 'Paid news by GPT-4',
        token_count: 456,
        cost: 0.08,
        latency: 8.3
      },
      'claude-3-sonnet': {
        status: 'complete',
        generated_text: 'Paid news by Claude',
        token_count: 350,
        cost: 0.05,
        latency: 5.2
      }
    },
    unpaid: {
      'gpt-4': {
        status: 'complete',
        generated_text: 'Free news by GPT-4',
        token_count: 280,
        cost: 0.04,
        latency: 6.1
      }
    }
  };

  test('renders results grouped by prompt type', () => {
    const mockGenerationContext = {
      results: mockResults,
      isGenerating: false,
      failedGenerations: 0,
      // ... other context values
    };

    render(
      <GenerationProvider value={mockGenerationContext}>
        <ResultsDisplay />
      </GenerationProvider>
    );

    expect(screen.getByText('üí∞ Paid')).toBeInTheDocument();
    expect(screen.getByText('üÜì Unpaid')).toBeInTheDocument();
  });

  test('collapses and expands prompt type sections', () => {
    // Mock GenerationContext setup...
    render(<ResultsDisplay />);

    const paidHeader = screen.getByText('üí∞ Paid');
    fireEvent.click(paidHeader);

    // After collapse, models should not be visible
    expect(screen.queryByText('gpt-4')).not.toBeInTheDocument();

    // Click again to expand
    fireEvent.click(paidHeader);
    expect(screen.getByText('gpt-4')).toBeInTheDocument();
  });

  test('displays total cost summary', () => {
    render(<ResultsDisplay />);

    // Total cost: 0.08 + 0.05 + 0.04 = 0.17
    expect(screen.getByText(/Total Cost Summary.*\$0.17/)).toBeInTheDocument();
  });

  test('displays model count badges', () => {
    render(<ResultsDisplay />);

    expect(screen.getByText('2 models')).toBeInTheDocument(); // Paid type
    expect(screen.getByText('1 models')).toBeInTheDocument(); // Unpaid type
  });

  test('shows responsive columns', () => {
    const { container } = render(<ResultsDisplay />);

    const cols = container.querySelectorAll('[class*="col-"]');
    expect(cols.length).toBeGreaterThan(0);
  });
});
```

**Manual Verification Checklist**:
1. Prompt type headers display with correct icons and colors (üí∞, üÜì, üï∑Ô∏è)
2. Models grouped under each prompt type and displayed side-by-side
3. Collapsible sections work (expand/collapse by clicking header)
4. Generated text displays with markdown formatting
5. Metadata section shows below each result with icons
6. Copy button copies text to clipboard with visual feedback
7. Outliers highlighted within each prompt type group
8. Desktop layout: 3 columns, Tablet: 2 columns, Mobile: 1 column
9. Total cost summary shows at bottom
10. Failed generations display error message
11. Responsive design verified on multiple screen sizes
12. Performance indicators color-coded (green/yellow/red)
13. Token breakdown tooltip shows prompt/completion tokens
14. Page remains responsive during generation
15. No console errors or warnings

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-29 | 1.0 | Initial story created from Epic 4 | Sarah (PO) |
| 2025-10-29 | 1.1 | Complete implementation with ResultsDisplay, ResultCard components, and testing | Dev Agent |

## Dev Agent Record

*To be populated during implementation*

### Agent Model Used

*To be filled by dev agent*

### Debug Log References

*To be filled by dev agent*

### Completion Notes List

*To be filled by dev agent*

### File List

*To be filled by dev agent*

## QA Results

*To be filled by QA agent*
