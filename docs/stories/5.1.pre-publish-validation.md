# Story 5.1: Pre-Publish Validation (All Prompt Types)

## Status

‚úÖ Completed (2025-11-07)

## Story

**As a** content manager,
**I want** the system to validate all prompt type configurations before publishing,
**so that** I don't accidentally deploy incomplete or untested configurations for any audience type.

## Acceptance Criteria

1. "Publish" button triggers validation checks for all three prompt types before allowing publication
2. Validation rules implemented per prompt type: prompt not empty, at least one successful test generation exists for each type
3. Shared validation: at least one API configured (FR5), section order defined, model selected
4. Validation failure displays modal with checklist of issues grouped by prompt type and prevents publishing
5. Validation success enables "Confirm Publish" button
6. Meets FR26 and FR31: validation of complete and tested configurations for all prompt types before publish
7. Visual checklist shows:
   - ‚úì APIs configured (shared)
   - ‚úì Section order defined (shared)
   - ‚úì Model selected (shared)
   - Per prompt type:
     - ‚úì Paid prompt created and tested
     - ‚úì Unpaid prompt created and tested
     - ‚úì Crawler prompt created and tested
8. Warning if any prompt type configuration differs from last successful test (prompt changed without re-testing)
9. Optional: Suggest minimum testing threshold (e.g., "Test all prompt types with at least 2 models before publishing")
10. Validation errors include actionable guidance per prompt type (e.g., "Paid prompt is empty - add prompt content to continue")

## Tasks / Subtasks

- [ ] Task 1: Create validation service for shared configuration (AC: 3)
  - [ ] Create backend/app/services/validation_service.py
  - [ ] Implement validate_shared_config method (APIs, sections, models)
  - [ ] Return structured validation results
  - [ ] Add error messages with actionable guidance
- [ ] Task 2: Create validation for all 3 prompt types (AC: 2, 8)
  - [ ] Implement validate_prompt_type method (paid, unpaid, crawler)
  - [ ] Check prompt not empty per type
  - [ ] Verify at least one test generation exists per type
  - [ ] Check if prompt changed since last test (warning)
  - [ ] Return validation results grouped by prompt type
- [ ] Task 3: Create validation endpoint (AC: 1, 4, 5)
  - [ ] Create POST /api/triggers/:id/validate endpoint
  - [ ] Call validation service for shared + all 3 types
  - [ ] Return comprehensive validation report
  - [ ] Include actionable guidance per issue
- [ ] Task 4: Create ValidationModal component (AC: 4, 5, 7, 10)
  - [ ] Create frontend/src/components/config/ValidationModal.tsx
  - [ ] Display validation checklist grouped by category
  - [ ] Show shared validation (APIs, sections, models)
  - [ ] Show per-type validation (paid, unpaid, crawler)
  - [ ] Display warnings for changed-but-untested prompts
  - [ ] Show actionable error messages
  - [ ] Enable "Confirm Publish" only when all validation passes
- [ ] Task 5: Integrate validation into publish flow (AC: 1, 6)
  - [ ] Add "Publish" button to Configuration Workspace
  - [ ] Trigger validation on publish click
  - [ ] Display ValidationModal with results
  - [ ] Prevent publishing if validation fails
  - [ ] Enable publish confirmation if validation succeeds
- [ ] Task 6: Write unit and integration tests
  - [ ] Test validation service for all 3 prompt types
  - [ ] Test validation endpoint
  - [ ] Test ValidationModal component
  - [ ] Test publish flow with validation
  - [ ] Test validation failure scenarios

## Dev Notes

### Prerequisites from Previous Stories

[Source: Story 4.5 - Completion]

Before starting this story, ensure Epic 4 is complete:
- Multi-model generation working for all 3 prompt types
- Test generation results stored in generation_history
- PromptEditor component with 3 tabs (paid, unpaid, crawler)
- Configuration contains all 3 prompt templates
- Model selection interface functional

### Project Structure & File Locations

[Source: architecture.md - Source Tree and Module Organization - Backend Module Structure]

**Backend files to create/modify** in `backend/app/`:
```
backend/app/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ validation_service.py          # Validation logic for all 3 types (NEW)
‚îÇ   ‚îî‚îÄ‚îÄ configuration_service.py       # Update with validation calls (MODIFY)
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îî‚îÄ‚îÄ configuration.py               # Add validation endpoint (MODIFY)
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ validation.py                  # Validation result models (NEW)
```

**Frontend files to create/modify** in `frontend/src/`:
```
frontend/src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ ValidationModal.tsx        # Validation results modal (NEW)
‚îÇ       ‚îú‚îÄ‚îÄ PublishButton.tsx          # Publish with validation (NEW)
‚îÇ       ‚îî‚îÄ‚îÄ ConfigurationWorkspace.tsx # Add publish button (MODIFY)
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ validation.ts                  # TypeScript validation types (NEW)
```

### Technology Stack Details

[Source: architecture.md - High Level Architecture - Planned Tech Stack]

**Backend Stack**:
- **FastAPI**: Validation endpoint
- **Pydantic**: Validation result models
- **MongoDB**: Query generation_history for test results

**Frontend Stack**:
- **React-Bootstrap**: Modal, checklist, alerts
- **TypeScript**: Type-safe validation results

### Validation Service Specification

[Source: architecture.md - Epic 5 - Publishing APIs]

**File: backend/app/services/validation_service.py** (New)

Complete validation service for all 3 prompt types:

```python
from typing import Dict, List, Optional, Any
from datetime import datetime
from motor.motor_asyncio import AsyncIOMotorDatabase
import logging

logger = logging.getLogger(__name__)

class ValidationResult:
    """Validation result for a single check"""
    def __init__(self, passed: bool, message: str, level: str = "error"):
        self.passed = passed
        self.message = message
        self.level = level  # "error" or "warning"

class ValidationReport:
    """Complete validation report for all prompt types"""
    def __init__(self):
        self.shared_checks: List[ValidationResult] = []
        self.prompt_checks: Dict[str, List[ValidationResult]] = {
            "paid": [],
            "unpaid": [],
            "crawler": []
        }
        self.all_passed: bool = False
        self.has_warnings: bool = False

    def to_dict(self) -> Dict:
        return {
            "all_passed": self.all_passed,
            "has_warnings": self.has_warnings,
            "shared_checks": [
                {"passed": c.passed, "message": c.message, "level": c.level}
                for c in self.shared_checks
            ],
            "prompt_checks": {
                prompt_type: [
                    {"passed": c.passed, "message": c.message, "level": c.level}
                    for c in checks
                ]
                for prompt_type, checks in self.prompt_checks.items()
            }
        }

class ValidationService:
    """Service for validating configurations before publishing"""

    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db

    async def validate_configuration(self, trigger_id: str) -> ValidationReport:
        """
        Validate complete configuration for all 3 prompt types

        Args:
            trigger_id: Trigger ID to validate

        Returns:
            ValidationReport with all validation results
        """
        report = ValidationReport()

        # Get configuration
        config = await self.db.configurations.find_one(
            {"trigger_id": trigger_id, "is_active": False},
            sort=[("updated_at", -1)]
        )

        if not config:
            report.shared_checks.append(
                ValidationResult(False, "No draft configuration found. Create a configuration first.")
            )
            report.all_passed = False
            return report

        # Validate shared configuration
        await self._validate_shared_config(config, report)

        # Validate each prompt type
        for prompt_type in ["paid", "unpaid", "crawler"]:
            await self._validate_prompt_type(config, prompt_type, report)

        # Determine overall pass/fail
        all_errors_passed = all(c.passed for c in report.shared_checks)
        for checks in report.prompt_checks.values():
            all_errors_passed = all_errors_passed and all(
                c.passed for c in checks if c.level == "error"
            )

        report.all_passed = all_errors_passed

        # Check for warnings
        has_warnings = any(c.level == "warning" and not c.passed for c in report.shared_checks)
        for checks in report.prompt_checks.values():
            has_warnings = has_warnings or any(
                c.level == "warning" and not c.passed for c in checks
            )
        report.has_warnings = has_warnings

        return report

    async def _validate_shared_config(self, config: Dict, report: ValidationReport):
        """Validate shared configuration (APIs, sections, models)"""

        # Check at least one API configured
        if not config.get("data_sections") or len(config["data_sections"]) == 0:
            report.shared_checks.append(
                ValidationResult(
                    False,
                    "No data sections configured. Add at least one data section in Data Configuration."
                )
            )
        else:
            report.shared_checks.append(
                ValidationResult(True, f"{len(config['data_sections'])} data sections configured")
            )

        # Check section order defined
        if not config.get("section_order") or len(config["section_order"]) == 0:
            report.shared_checks.append(
                ValidationResult(
                    False,
                    "Section order not defined. Reorder sections in Section Management."
                )
            )
        else:
            report.shared_checks.append(
                ValidationResult(True, f"Section order defined ({len(config['section_order'])} sections)")
            )

        # Check model selected
        model_config = config.get("model_config", {})
        selected_models = model_config.get("selected_models", [])

        if not selected_models or len(selected_models) == 0:
            report.shared_checks.append(
                ValidationResult(
                    False,
                    "No LLM model selected. Select at least one model in Model Selection."
                )
            )
        else:
            report.shared_checks.append(
                ValidationResult(True, f"{len(selected_models)} models selected")
            )

    async def _validate_prompt_type(
        self,
        config: Dict,
        prompt_type: str,
        report: ValidationReport
    ):
        """Validate specific prompt type configuration"""

        prompts = config.get("prompts", {})
        prompt_config = prompts.get(prompt_type)

        # Check if prompt exists and not empty
        if not prompt_config or not prompt_config.get("template"):
            report.prompt_checks[prompt_type].append(
                ValidationResult(
                    False,
                    f"{prompt_type.capitalize()} prompt is empty. Add prompt content in Prompt Editor."
                )
            )
            return  # Skip further checks if prompt empty

        report.prompt_checks[prompt_type].append(
            ValidationResult(
                True,
                f"{prompt_type.capitalize()} prompt created ({len(prompt_config['template'])} characters)"
            )
        )

        # Check for at least one test generation
        last_test = prompt_config.get("last_test_generation")

        if not last_test:
            report.prompt_checks[prompt_type].append(
                ValidationResult(
                    False,
                    f"{prompt_type.capitalize()} prompt not tested. Run test generation before publishing."
                )
            )
            return

        report.prompt_checks[prompt_type].append(
            ValidationResult(
                True,
                f"{prompt_type.capitalize()} prompt tested on {last_test.get('timestamp', 'unknown')}"
            )
        )

        # Warning: Check if prompt changed since last test
        version_history = prompt_config.get("version_history", [])
        if version_history and len(version_history) > 0:
            last_version = version_history[-1]
            last_test_template = last_test.get("prompt_template")

            if last_test_template and last_version.get("template") != last_test_template:
                report.prompt_checks[prompt_type].append(
                    ValidationResult(
                        False,
                        f"WARNING: {prompt_type.capitalize()} prompt changed since last test. "
                        f"Consider re-testing before publishing.",
                        level="warning"
                    )
                )
```

**File: backend/app/routers/configuration.py** (Update)

Add validation endpoint:

```python
from fastapi import APIRouter, HTTPException, Depends
from ..database import get_database
from ..services.validation_service import ValidationService
import logging

router = APIRouter(prefix="/api/triggers/{trigger_id}/config", tags=["configuration"])
logger = logging.getLogger(__name__)

# ... (existing endpoints)

@router.post("/validate")
async def validate_configuration(
    trigger_id: str,
    db = Depends(get_database)
):
    """
    Validate configuration before publishing.
    Checks shared config (APIs, sections, models) and all 3 prompt types.
    """
    service = ValidationService(db)

    try:
        report = await service.validate_configuration(trigger_id)

        return {
            "success": True,
            "validation": report.to_dict()
        }
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        raise HTTPException(status_code=400, detail=str(e))
```

### Frontend Component Specifications

[Source: architecture.md - Source Tree and Module Organization - Frontend Module Structure]

**File: frontend/src/components/config/ValidationModal.tsx** (New)

Complete validation modal with checklist:

```typescript
'use client';

import { Modal, Button, Alert, ListGroup, Badge } from 'react-bootstrap';

interface ValidationCheck {
  passed: boolean;
  message: string;
  level: 'error' | 'warning';
}

interface ValidationReport {
  all_passed: boolean;
  has_warnings: boolean;
  shared_checks: ValidationCheck[];
  prompt_checks: {
    paid: ValidationCheck[];
    unpaid: ValidationCheck[];
    crawler: ValidationCheck[];
  };
}

interface ValidationModalProps {
  show: boolean;
  onHide: () => void;
  validation: ValidationReport | null;
  onConfirmPublish: () => void;
  loading: boolean;
}

export default function ValidationModal({
  show,
  onHide,
  validation,
  onConfirmPublish,
  loading
}: ValidationModalProps) {
  if (!validation) return null;

  const renderCheckIcon = (check: ValidationCheck) => {
    if (check.passed) {
      return <i className="bi bi-check-circle-fill text-success me-2"></i>;
    }
    if (check.level === 'warning') {
      return <i className="bi bi-exclamation-triangle-fill text-warning me-2"></i>;
    }
    return <i className="bi bi-x-circle-fill text-danger me-2"></i>;
  };

  const promptTypeEmoji = {
    paid: 'üí∞',
    unpaid: 'üÜì',
    crawler: 'üï∑Ô∏è'
  };

  const promptTypeColor = {
    paid: '#0d6efd',
    unpaid: '#198754',
    crawler: '#fd7e14'
  };

  return (
    <Modal show={show} onHide={onHide} size="lg">
      <Modal.Header closeButton>
        <Modal.Title>
          {validation.all_passed ? '‚úÖ Validation Passed' : '‚ùå Validation Failed'}
        </Modal.Title>
      </Modal.Header>
      <Modal.Body>
        {validation.all_passed ? (
          <Alert variant="success">
            All validation checks passed! Your configuration is ready to publish.
          </Alert>
        ) : (
          <Alert variant="danger">
            Some validation checks failed. Please fix the issues below before publishing.
          </Alert>
        )}

        {validation.has_warnings && (
          <Alert variant="warning">
            <i className="bi bi-exclamation-triangle-fill me-2"></i>
            There are warnings. Review them before proceeding.
          </Alert>
        )}

        {/* Shared Configuration Checks */}
        <h5 className="mt-4 mb-3">Shared Configuration</h5>
        <ListGroup className="mb-4">
          {validation.shared_checks.map((check, index) => (
            <ListGroup.Item
              key={index}
              className={`d-flex align-items-start ${
                !check.passed && check.level === 'error' ? 'border-danger' : ''
              }`}
            >
              {renderCheckIcon(check)}
              <div className="flex-grow-1">{check.message}</div>
            </ListGroup.Item>
          ))}
        </ListGroup>

        {/* Per-Prompt Type Checks */}
        {Object.entries(validation.prompt_checks).map(([promptType, checks]) => (
          <div key={promptType}>
            <h5 className="mb-3" style={{ color: promptTypeColor[promptType as keyof typeof promptTypeColor] }}>
              {promptTypeEmoji[promptType as keyof typeof promptTypeEmoji]} {promptType.charAt(0).toUpperCase() + promptType.slice(1)} Prompt
            </h5>
            <ListGroup className="mb-4">
              {checks.map((check, index) => (
                <ListGroup.Item
                  key={index}
                  className={`d-flex align-items-start ${
                    !check.passed && check.level === 'error' ? 'border-danger' : ''
                  } ${
                    !check.passed && check.level === 'warning' ? 'border-warning' : ''
                  }`}
                >
                  {renderCheckIcon(check)}
                  <div className="flex-grow-1">{check.message}</div>
                </ListGroup.Item>
              ))}
            </ListGroup>
          </div>
        ))}

        {validation.all_passed && (
          <Alert variant="info" className="mt-4">
            <strong>Next Step:</strong> Click "Confirm Publish" to review final configuration and publish to production.
          </Alert>
        )}
      </Modal.Body>
      <Modal.Footer>
        <Button variant="secondary" onClick={onHide} disabled={loading}>
          Cancel
        </Button>
        <Button
          variant="primary"
          onClick={onConfirmPublish}
          disabled={!validation.all_passed || loading}
        >
          {loading ? 'Publishing...' : 'Confirm Publish'}
        </Button>
      </Modal.Footer>
    </Modal>
  );
}
```

**File: frontend/src/components/config/PublishButton.tsx** (New)

Publish button with validation flow:

```typescript
'use client';

import { useState } from 'react';
import { Button } from 'react-bootstrap';
import ValidationModal from './ValidationModal';

interface PublishButtonProps {
  triggerId: string;
  onPublishSuccess: () => void;
}

export default function PublishButton({ triggerId, onPublishSuccess }: PublishButtonProps) {
  const [showValidation, setShowValidation] = useState(false);
  const [validation, setValidation] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const handlePublishClick = async () => {
    try {
      setLoading(true);
      setError(null);

      // Trigger validation
      const response = await fetch(`/api/triggers/${triggerId}/config/validate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' }
      });

      if (!response.ok) {
        throw new Error('Validation failed');
      }

      const data = await response.json();
      setValidation(data.validation);
      setShowValidation(true);
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Validation failed');
    } finally {
      setLoading(false);
    }
  };

  const handleConfirmPublish = () => {
    // This will be implemented in Story 5.2
    // For now, just close modal
    setShowValidation(false);
    onPublishSuccess();
  };

  return (
    <>
      <Button
        variant="success"
        size="lg"
        onClick={handlePublishClick}
        disabled={loading}
      >
        {loading ? 'Validating...' : 'Publish Configuration'}
      </Button>

      {error && (
        <div className="alert alert-danger mt-3">{error}</div>
      )}

      <ValidationModal
        show={showValidation}
        onHide={() => setShowValidation(false)}
        validation={validation}
        onConfirmPublish={handleConfirmPublish}
        loading={loading}
      />
    </>
  );
}
```

### Implementation Notes

[Source: PRD - Epic 5: Configuration Publishing & Production Integration]

**Important Design Decisions**:

1. **All 3 Prompt Types Validated**: Validation checks ALL 3 prompt types (paid, unpaid, crawler) before allowing publish.

2. **Shared vs Per-Type Validation**: Shared config (APIs, sections, models) validated once. Prompt-specific validation (empty check, test generation) validated per type.

3. **Error vs Warning Levels**: Errors block publishing. Warnings allow publishing but alert user to potential issues.

4. **Actionable Guidance**: Each validation error includes specific guidance on how to fix (e.g., "Add prompt content in Prompt Editor").

5. **Test Generation Check**: Validates that each prompt type has been tested at least once. Warns if prompt changed since last test.

6. **Grouped Display**: ValidationModal groups checks by category (Shared ‚Üí Paid ‚Üí Unpaid ‚Üí Crawler) for clarity.

7. **Publish Flow**: Publish button ‚Üí Validation ‚Üí ValidationModal ‚Üí Confirm Publish (Story 5.2).

### Testing

[Source: PRD - Technical Assumptions - Testing Requirements]

**Testing Strategy for Story 5.1**:

**Unit Tests** (backend/tests/test_validation_service.py):

```python
import pytest
from app.services.validation_service import ValidationService, ValidationReport

@pytest.mark.asyncio
async def test_validation_service_shared_config(db):
    """Test shared configuration validation"""
    # Create config with missing APIs
    await db.configurations.insert_one({
        "_id": "config_001",
        "trigger_id": "trigger_001",
        "data_sections": [],
        "section_order": [],
        "model_config": {"selected_models": []},
        "prompts": {},
        "is_active": False
    })

    service = ValidationService(db)
    report = await service.validate_configuration("trigger_001")

    assert not report.all_passed
    assert any("No data sections" in c.message for c in report.shared_checks)
    assert any("Section order not defined" in c.message for c in report.shared_checks)
    assert any("No LLM model selected" in c.message for c in report.shared_checks)

@pytest.mark.asyncio
async def test_validation_service_prompt_types(db):
    """Test all 3 prompt type validation"""
    await db.configurations.insert_one({
        "_id": "config_002",
        "trigger_id": "trigger_002",
        "data_sections": ["1", "2"],
        "section_order": ["1", "2"],
        "model_config": {"selected_models": ["gpt-4"]},
        "prompts": {
            "paid": {"template": "", "version_history": [], "last_test_generation": None},
            "unpaid": {"template": "test", "version_history": [], "last_test_generation": None},
            "crawler": {"template": "test", "version_history": [], "last_test_generation": {"timestamp": "2025-10-29"}}
        },
        "is_active": False
    })

    service = ValidationService(db)
    report = await service.validate_configuration("trigger_002")

    assert not report.all_passed
    # Paid prompt empty
    assert any("Paid prompt is empty" in c.message for c in report.prompt_checks["paid"])
    # Unpaid not tested
    assert any("Unpaid prompt not tested" in c.message for c in report.prompt_checks["unpaid"])
    # Crawler passed
    assert all(c.passed for c in report.prompt_checks["crawler"])

@pytest.mark.asyncio
async def test_validation_changed_prompt_warning(db):
    """Test warning when prompt changed since last test"""
    await db.configurations.insert_one({
        "_id": "config_003",
        "trigger_id": "trigger_003",
        "data_sections": ["1"],
        "section_order": ["1"],
        "model_config": {"selected_models": ["gpt-4"]},
        "prompts": {
            "paid": {
                "template": "new prompt",
                "version_history": [{"template": "new prompt", "timestamp": "2025-10-29"}],
                "last_test_generation": {
                    "timestamp": "2025-10-28",
                    "prompt_template": "old prompt"
                }
            }
        },
        "is_active": False
    })

    service = ValidationService(db)
    report = await service.validate_configuration("trigger_003")

    assert report.has_warnings
    assert any(
        "changed since last test" in c.message
        for c in report.prompt_checks["paid"]
        if c.level == "warning"
    )

@pytest.mark.asyncio
async def test_validation_all_passed(db):
    """Test validation passes with complete configuration"""
    await db.configurations.insert_one({
        "_id": "config_004",
        "trigger_id": "trigger_004",
        "data_sections": ["1", "2"],
        "section_order": ["1", "2"],
        "model_config": {"selected_models": ["gpt-4"]},
        "prompts": {
            "paid": {
                "template": "paid prompt",
                "version_history": [],
                "last_test_generation": {"timestamp": "2025-10-29", "prompt_template": "paid prompt"}
            },
            "unpaid": {
                "template": "unpaid prompt",
                "version_history": [],
                "last_test_generation": {"timestamp": "2025-10-29", "prompt_template": "unpaid prompt"}
            },
            "crawler": {
                "template": "crawler prompt",
                "version_history": [],
                "last_test_generation": {"timestamp": "2025-10-29", "prompt_template": "crawler prompt"}
            }
        },
        "is_active": False
    })

    service = ValidationService(db)
    report = await service.validate_configuration("trigger_004")

    assert report.all_passed
    assert all(c.passed for c in report.shared_checks)
    assert all(c.passed for c in report.prompt_checks["paid"] if c.level == "error")
    assert all(c.passed for c in report.prompt_checks["unpaid"] if c.level == "error")
    assert all(c.passed for c in report.prompt_checks["crawler"] if c.level == "error")
```

**Frontend Tests** (frontend/__tests__/components/config/):

```typescript
// ValidationModal.test.tsx
import { render, screen, fireEvent } from '@testing-library/react';
import ValidationModal from '@/components/config/ValidationModal';

describe('ValidationModal Component', () => {
  const mockValidationPassed = {
    all_passed: true,
    has_warnings: false,
    shared_checks: [
      { passed: true, message: '2 data sections configured', level: 'error' as const },
      { passed: true, message: 'Section order defined (2 sections)', level: 'error' as const },
      { passed: true, message: '1 models selected', level: 'error' as const }
    ],
    prompt_checks: {
      paid: [
        { passed: true, message: 'Paid prompt created (100 characters)', level: 'error' as const },
        { passed: true, message: 'Paid prompt tested on 2025-10-29', level: 'error' as const }
      ],
      unpaid: [
        { passed: true, message: 'Unpaid prompt created (80 characters)', level: 'error' as const },
        { passed: true, message: 'Unpaid prompt tested on 2025-10-29', level: 'error' as const }
      ],
      crawler: [
        { passed: true, message: 'Crawler prompt created (90 characters)', level: 'error' as const },
        { passed: true, message: 'Crawler prompt tested on 2025-10-29', level: 'error' as const }
      ]
    }
  };

  const mockValidationFailed = {
    all_passed: false,
    has_warnings: false,
    shared_checks: [
      { passed: false, message: 'No data sections configured', level: 'error' as const }
    ],
    prompt_checks: {
      paid: [
        { passed: false, message: 'Paid prompt is empty', level: 'error' as const }
      ],
      unpaid: [],
      crawler: []
    }
  };

  test('renders validation passed state', () => {
    render(
      <ValidationModal
        show={true}
        onHide={jest.fn()}
        validation={mockValidationPassed}
        onConfirmPublish={jest.fn()}
        loading={false}
      />
    );

    expect(screen.getByText(/Validation Passed/)).toBeInTheDocument();
    expect(screen.getByText(/All validation checks passed/)).toBeInTheDocument();
  });

  test('renders validation failed state', () => {
    render(
      <ValidationModal
        show={true}
        onHide={jest.fn()}
        validation={mockValidationFailed}
        onConfirmPublish={jest.fn()}
        loading={false}
      />
    );

    expect(screen.getByText(/Validation Failed/)).toBeInTheDocument();
    expect(screen.getByText(/No data sections configured/)).toBeInTheDocument();
    expect(screen.getByText(/Paid prompt is empty/)).toBeInTheDocument();
  });

  test('confirm publish button enabled only when validation passes', () => {
    const { rerender } = render(
      <ValidationModal
        show={true}
        onHide={jest.fn()}
        validation={mockValidationFailed}
        onConfirmPublish={jest.fn()}
        loading={false}
      />
    );

    const confirmButton = screen.getByText('Confirm Publish');
    expect(confirmButton).toBeDisabled();

    rerender(
      <ValidationModal
        show={true}
        onHide={jest.fn()}
        validation={mockValidationPassed}
        onConfirmPublish={jest.fn()}
        loading={false}
      />
    );

    expect(confirmButton).not.toBeDisabled();
  });

  test('displays all 3 prompt type sections', () => {
    render(
      <ValidationModal
        show={true}
        onHide={jest.fn()}
        validation={mockValidationPassed}
        onConfirmPublish={jest.fn()}
        loading={false}
      />
    );

    expect(screen.getByText(/üí∞ Paid Prompt/)).toBeInTheDocument();
    expect(screen.getByText(/üÜì Unpaid Prompt/)).toBeInTheDocument();
    expect(screen.getByText(/üï∑Ô∏è Crawler Prompt/)).toBeInTheDocument();
  });
});
```

**Manual Verification**:
1. Publish button triggers validation for all 3 prompt types
2. Validation modal displays grouped checklist (shared + 3 types)
3. Shared validation checks: APIs, sections, models
4. Per-type validation checks: prompt not empty, test generation exists
5. Validation failure blocks publish with actionable error messages
6. Validation success enables "Confirm Publish" button
7. Warning displayed if prompt changed since last test
8. All checks displayed with appropriate icons (‚úì, ‚ö†, ‚úó)
9. Modal responsive and readable on desktop/tablet
10. No console errors

**Coverage Target**: 70%+ for validation logic

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-29 | 1.0 | Initial story created from Epic 5 | Sarah (PO) |
| 2025-10-29 | 1.1 | Enriched with full architectural context, validation service specs for all 3 prompt types, and testing standards | Bob (SM) |

## Dev Agent Record

*To be populated during implementation*

### Agent Model Used

*To be filled by dev agent*

### Debug Log References

*To be filled by dev agent*

### Completion Notes List

*To be filled by dev agent*

### File List

*To be filled by dev agent*

## QA Results

*To be filled by QA agent*
