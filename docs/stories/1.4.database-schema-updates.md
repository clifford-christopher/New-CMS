# Story 1.4: Database Schema Updates for News CMS Workflow

## Status

To Do

## Story

**As a** developer,
**I want** MongoDB collections extended to support workflow configuration data,
**so that** the system can persist trigger configurations with data modes, section selections, and multi-type prompts.

## Acceptance Criteria

1. Extend `trigger_prompts` collection schema with new fields:
   - `isActive` (boolean, default: false)
   - `model_config` (object: provider, model, temperature)
   - `data_config` (object: data_mode ["old"|"new"|"old_new"], sections [array], section_order [array])
   - `prompts` (object: paid, unpaid, crawler - each with template string)
   - `version` (integer, auto-increment)
   - `updated_at` (datetime)
2. Pydantic models created for:
   - `TriggerPromptConfig` (full configuration)
   - `DataConfig` (data_mode, sections, section_order)
   - `ModelConfig` (provider, model, temperature, max_tokens)
   - `PromptTemplates` (paid, unpaid, crawler)
3. Create `generation_history` collection schema:
   - `trigger_name`, `stockid`, `data_mode`, `prompt_type`, `model`, `timestamp`
   - `input_data`, `generated_html`, `extracted_title`, `extracted_summary`, `extracted_article`
   - `tokens_used`, `cost`, `generation_time`
4. Create `prompt_versions` collection for rollback capability:
   - `trigger_name`, `version`, `prompts`, `model_config`, `data_config`, `published_at`, `published_by`
5. Migration script or documentation creates indexes:
   - `trigger_prompts`: trigger_name (unique), isActive
   - `generation_history`: trigger_name + stockid + timestamp, prompt_type
   - `prompt_versions`: trigger_name + version
6. Validation rules enforced at Pydantic level:
   - data_mode must be one of ["old", "new", "old_new"]
   - sections must be array of integers 1-14, no duplicates
   - section_order must match length of sections
   - prompts.paid is required, prompts.unpaid and prompts.crawler optional
7. Meets NFR11: Backward compatibility - existing trigger_prompts documents without new fields still work

## Tasks / Subtasks

- [ ] Task 1: Create Pydantic models for News CMS Workflow (AC: 2)
  - [ ] Create `backend/app/models/trigger_prompt_config.py` with TriggerPromptConfig model
  - [ ] Create DataConfig model with validation (data_mode, sections, section_order)
  - [ ] Create ModelConfig model with provider enum and settings
  - [ ] Create PromptTemplates model with paid (required), unpaid, crawler (optional)
  - [ ] Write unit tests for Pydantic validation rules
- [ ] Task 2: Extend trigger_prompts collection schema (AC: 1)
  - [ ] Update TriggerPromptConfig model to include new fields
  - [ ] Add default values: isActive=false, version=1
  - [ ] Ensure backward compatibility (existing docs without new fields remain valid)
  - [ ] Document schema changes in migration guide
- [ ] Task 3: Create generation_history collection schema (AC: 3)
  - [ ] Create `GenerationHistory` Pydantic model
  - [ ] Include all required fields (trigger_name, stockid, data_mode, prompt_type, model, etc.)
  - [ ] Add metadata fields (tokens_used, cost, generation_time, timestamp, status)
  - [ ] Write unit tests for model validation
- [ ] Task 4: Create prompt_versions collection schema (AC: 4)
  - [ ] Create `PromptVersion` Pydantic model
  - [ ] Store full snapshot of configuration at publish time
  - [ ] Add audit metadata (published_at, published_by, test_generation_count)
  - [ ] Write unit tests for model
- [ ] Task 5: Create MongoDB indexes (AC: 5)
  - [ ] Write migration script `backend/scripts/create_indexes.py`
  - [ ] Create unique index on trigger_prompts.trigger_name
  - [ ] Create index on trigger_prompts.isActive
  - [ ] Create compound index on generation_history (trigger_name + stockid + timestamp)
  - [ ] Create index on generation_history.prompt_type
  - [ ] Create compound index on prompt_versions (trigger_name + version)
  - [ ] Document index rationale in migration guide
- [ ] Task 6: Implement validation rules (AC: 6)
  - [ ] Validate data_mode enum ("old", "new", "old_new")
  - [ ] Validate sections array (1-14, no duplicates)
  - [ ] Validate section_order matches sections length and values
  - [ ] Validate prompts.paid is required
  - [ ] Write comprehensive unit tests for all validation scenarios
- [ ] Task 7: Test backward compatibility (AC: 7)
  - [ ] Insert test document without new fields into trigger_prompts
  - [ ] Query document with TriggerPromptConfig model (should not fail)
  - [ ] Verify default values applied (isActive=false)
  - [ ] Test with existing news generation queries
  - [ ] Document backward compatibility guarantees

## Dev Notes

### Prerequisites from Story 1.2

[Source: Story 1.2 - MongoDB Database Setup and Connection]

Before starting this story, ensure Story 1.2 is complete:
- MongoDB connection established via Motor driver
- Database health check endpoint `/api/health` working
- Pydantic models pattern established
- Environment variables configured for MongoDB URI

### Architecture Context

[Source: docs/architecture/news-cms-workflow.md - Database Schema]

**Key Design Decisions**:
1. **Extend Existing Collection**: Extend `trigger_prompts` collection rather than create new collection (per user requirement)
2. **No Caching**: Collection data not cached (except prompts with 5-min TTL) due to dynamic nature
3. **stockid-Specific**: All data operations require both trigger_name AND stockid parameters
4. **Backward Compatibility**: Existing documents without new fields continue to work (isActive defaults to false → legacy method)

### Pydantic Models Structure

[Source: docs/architecture/news-cms-workflow.md - Database Schema]

**File: backend/app/models/trigger_prompt_config.py**

```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Literal
from datetime import datetime
from enum import Enum

class LLMProvider(str, Enum):
    """Supported LLM providers."""
    OPENAI = "openai"
    CLAUDE = "claude"

class DataMode(str, Enum):
    """Data modes for news generation."""
    OLD = "old"  # Existing trigger data from news_triggers
    NEW = "new"  # Generated structured data from generate_full_report.py
    OLD_NEW = "old_new"  # Merge of both

class ModelConfig(BaseModel):
    """LLM model configuration."""
    provider: LLMProvider
    model: str  # e.g., "claude-sonnet-4-5-20250929", "gpt-4o"
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(default=20000, gt=0)

class DataConfig(BaseModel):
    """Data source configuration."""
    data_mode: DataMode
    sections: Optional[List[int]] = Field(default=None)  # 1-14, only if NEW or OLD_NEW
    section_order: Optional[List[int]] = Field(default=None)  # Must match sections

    @validator('sections')
    def validate_sections(cls, v):
        """Validate sections are 1-14, no duplicates."""
        if v is not None:
            if not all(1 <= s <= 14 for s in v):
                raise ValueError("Sections must be integers between 1 and 14")
            if len(v) != len(set(v)):
                raise ValueError("Sections must not contain duplicates")
        return v

    @validator('section_order')
    def validate_section_order(cls, v, values):
        """Validate section_order matches sections."""
        if v is not None and 'sections' in values and values['sections'] is not None:
            if sorted(v) != sorted(values['sections']):
                raise ValueError("section_order must contain same values as sections")
        return v

class PromptTemplates(BaseModel):
    """Multi-type prompt templates."""
    paid: str = Field(..., min_length=1)  # Required
    unpaid: Optional[str] = None
    crawler: Optional[str] = None

class TriggerPromptConfig(BaseModel):
    """Extended trigger_prompts collection model."""
    trigger_name: str
    isActive: bool = False
    model_config: Optional[ModelConfig] = None
    data_config: Optional[DataConfig] = None
    prompts: Optional[PromptTemplates] = None
    version: int = 1
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    published_at: Optional[datetime] = None
    published_by: Optional[str] = None

    class Config:
        # Allow backward compatibility with existing documents
        # Documents without new fields will use default values
        allow_population_by_field_name = True
        validate_assignment = True
```

**File: backend/app/models/generation_history.py**

```python
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime

class GenerationHistory(BaseModel):
    """Generation history for preview and production news."""
    trigger_name: str
    stockid: int
    prompt_type: str  # "paid" | "unpaid" | "crawler"
    data_mode: str  # "old" | "new" | "old_new"
    model: str  # e.g., "claude-sonnet-4-5-20250929"

    # Input data
    input_data: Dict[str, Any]  # Merged data used for generation
    prompt_used: str  # Final prompt with substitutions

    # Output
    generated_html: str  # Full HTML output from LLM
    extracted_title: str
    extracted_summary: str
    extracted_article: str

    # Metadata
    method: str  # "new" | "legacy"
    tokens_used: int
    cost: float
    generation_time: float  # seconds
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    status: str  # "success" | "failed"
    error_message: Optional[str] = None
```

**File: backend/app/models/prompt_version.py**

```python
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime

class PromptVersion(BaseModel):
    """Version history for trigger prompt configurations."""
    trigger_name: str
    version: int

    # Full snapshot of configuration at publish time
    model_config: Dict[str, Any]
    data_config: Dict[str, Any]
    prompts: Dict[str, Any]  # All 3 types

    published_at: datetime = Field(default_factory=datetime.utcnow)
    published_by: str

    # Audit metadata
    test_generation_count: Optional[int] = 0
    avg_cost_per_generation: Optional[float] = 0.0
    iteration_count: Optional[int] = 0
```

### MongoDB Indexes

[Source: docs/architecture/news-cms-workflow.md - Database Schema]

**File: backend/scripts/create_indexes.py**

```python
import asyncio
from motor.motor_asyncio import AsyncIOMotorClient
import os

async def create_indexes():
    """Create MongoDB indexes for News CMS Workflow collections."""
    uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/news_cms")
    client = AsyncIOMotorClient(uri)
    db = client.get_default_database()

    print("Creating indexes for trigger_prompts collection...")
    await db.trigger_prompts.create_index("trigger_name", unique=True)
    await db.trigger_prompts.create_index("isActive")
    print("✓ trigger_prompts indexes created")

    print("Creating indexes for generation_history collection...")
    await db.generation_history.create_index([
        ("trigger_name", 1),
        ("stockid", 1),
        ("timestamp", -1)
    ])
    await db.generation_history.create_index("prompt_type")
    await db.generation_history.create_index([("timestamp", -1)])
    print("✓ generation_history indexes created")

    print("Creating indexes for prompt_versions collection...")
    await db.prompt_versions.create_index([
        ("trigger_name", 1),
        ("version", -1)
    ])
    print("✓ prompt_versions indexes created")

    print("\nAll indexes created successfully!")
    client.close()

if __name__ == "__main__":
    asyncio.run(create_indexes())
```

**Run with**: `python backend/scripts/create_indexes.py`

### Validation Rules Details

[Source: docs/architecture/news-cms-workflow.md - Database Schema]

**Critical Validation Rules**:

1. **data_mode**: Must be one of ["old", "new", "old_new"]
   - Enforced by Pydantic Enum `DataMode`
   - Invalid values raise ValidationError

2. **sections**: Must be array of integers 1-14, no duplicates
   - Validator: `validate_sections` checks range and uniqueness
   - Example valid: `[1, 2, 3, 5, 7, 9, 12]`
   - Example invalid: `[0, 1, 15]` (out of range), `[1, 1, 2]` (duplicates)

3. **section_order**: Must match sections length and contain same values
   - Validator: `validate_section_order` compares sorted arrays
   - Example: if sections = `[1, 3, 2]`, section_order could be `[1, 2, 3]` or `[3, 1, 2]`
   - Invalid: sections = `[1, 2, 3]`, section_order = `[1, 2]` (length mismatch)

4. **prompts.paid**: Required (min_length=1)
   - Cannot be empty string or None
   - unpaid and crawler are optional

### Backward Compatibility Strategy

[Source: docs/architecture/news-cms-workflow.md - Backward Compatibility Checklist]

**Approach**:
- Existing `trigger_prompts` documents without new fields continue to work
- Pydantic model provides default values:
  - `isActive` defaults to `False` → uses legacy generation method
  - `model_config`, `data_config`, `prompts` default to `None`
  - `version` defaults to `1`

**Testing**:
```python
# Test: Query existing document without new fields
existing_doc = await db.trigger_prompts.find_one({"trigger_name": "earnings_result"})
# Result: {"trigger_name": "earnings_result", ...existing fields...}

# Parse with Pydantic model
config = TriggerPromptConfig(**existing_doc)
# Result: config.isActive == False (default value)
# News generation service checks: if not config.isActive → use legacy method
```

### Database Migration Guide

**Step 1: Update Pydantic Models** (This Story)
- Add new models with default values
- Ensure existing documents parse successfully

**Step 2: Create Indexes** (This Story)
- Run `create_indexes.py` script on staging database
- Verify indexes created: `db.trigger_prompts.getIndexes()`

**Step 3: Gradual Rollout** (Story 1.15)
- Phase 1: Deploy backend with new models (no active configs yet)
- Phase 2: Test with 1-2 triggers (set isActive=true)
- Phase 3: Migrate remaining triggers incrementally

**Step 4: Monitor** (Story 1.16)
- Track generation_history to verify both methods working
- Monitor costs, errors, performance

### Testing Strategy

[Source: PRD - Technical Assumptions - Testing Requirements]

**Unit Tests** (backend/tests/models/):

```python
# test_trigger_prompt_config.py
import pytest
from app.models.trigger_prompt_config import TriggerPromptConfig, DataConfig, DataMode

def test_data_config_validation_sections_in_range():
    """Test sections must be 1-14."""
    with pytest.raises(ValueError, match="between 1 and 14"):
        DataConfig(data_mode=DataMode.NEW, sections=[0, 1, 15])

def test_data_config_validation_no_duplicates():
    """Test sections must not have duplicates."""
    with pytest.raises(ValueError, match="must not contain duplicates"):
        DataConfig(data_mode=DataMode.NEW, sections=[1, 1, 2])

def test_section_order_must_match_sections():
    """Test section_order must contain same values as sections."""
    with pytest.raises(ValueError, match="must contain same values"):
        DataConfig(
            data_mode=DataMode.NEW,
            sections=[1, 2, 3],
            section_order=[1, 2]  # Missing 3
        )

def test_prompts_paid_required():
    """Test prompts.paid is required."""
    from app.models.trigger_prompt_config import PromptTemplates
    with pytest.raises(ValueError):
        PromptTemplates(paid="")  # Empty string not allowed

def test_backward_compatibility():
    """Test existing documents without new fields parse successfully."""
    existing_doc = {
        "trigger_name": "earnings_result",
        # No isActive, model_config, data_config, prompts fields
    }
    config = TriggerPromptConfig(**existing_doc)
    assert config.isActive == False  # Default value
    assert config.model_config is None
    assert config.version == 1
```

**Integration Tests** (backend/tests/integration/):

```python
# test_database_schema.py
import pytest
from motor.motor_asyncio import AsyncIOMotorClient

@pytest.mark.asyncio
async def test_trigger_prompts_indexes(db):
    """Test trigger_prompts indexes created correctly."""
    indexes = await db.trigger_prompts.list_indexes().to_list(length=None)
    index_names = [idx['name'] for idx in indexes]

    assert 'trigger_name_1' in index_names  # Unique index
    assert 'isActive_1' in index_names

@pytest.mark.asyncio
async def test_generation_history_indexes(db):
    """Test generation_history indexes created correctly."""
    indexes = await db.generation_history.list_indexes().to_list(length=None)
    index_names = [idx['name'] for idx in indexes]

    assert 'trigger_name_1_stockid_1_timestamp_-1' in index_names  # Compound index
    assert 'prompt_type_1' in index_names

@pytest.mark.asyncio
async def test_insert_and_query_with_new_schema(db):
    """Test inserting document with new schema and querying."""
    config = {
        "trigger_name": "test_trigger",
        "isActive": True,
        "model_config": {
            "provider": "claude",
            "model": "claude-sonnet-4-5-20250929",
            "temperature": 0.7,
            "max_tokens": 20000
        },
        "data_config": {
            "data_mode": "new",
            "sections": [1, 2, 3],
            "section_order": [1, 2, 3]
        },
        "prompts": {
            "paid": "Generate detailed article...",
            "unpaid": "Generate brief summary...",
            "crawler": "Generate SEO content..."
        },
        "version": 1
    }

    result = await db.trigger_prompts.insert_one(config)
    assert result.inserted_id is not None

    # Query back
    found = await db.trigger_prompts.find_one({"trigger_name": "test_trigger"})
    assert found["isActive"] == True
    assert found["model_config"]["provider"] == "claude"
```

**Manual Verification Checklist**:
1. Run migration script successfully: `python backend/scripts/create_indexes.py`
2. Check indexes created: `mongosh` → `db.trigger_prompts.getIndexes()`
3. Insert test document with new schema via mongosh
4. Query document with Pydantic model (no exceptions)
5. Insert existing document without new fields
6. Verify default values applied (isActive=false)
7. Check unique index on trigger_name prevents duplicates

**Coverage Target**: 80%+ for models

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-30 | 1.0 | Initial story created from Epic 6 | Dev Agent |

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References

None

### Completion Notes List

(To be filled when story is completed)

### File List

**To Be Created:**
- backend/app/models/trigger_prompt_config.py
- backend/app/models/generation_history.py
- backend/app/models/prompt_version.py
- backend/scripts/create_indexes.py
- backend/tests/models/test_trigger_prompt_config.py
- backend/tests/models/test_generation_history.py
- backend/tests/models/test_prompt_version.py
- backend/tests/integration/test_database_schema.py

**To Be Modified:**
- None (extends existing collections without breaking changes)

## QA Results

(To be filled by QA after story completion)
